:_content-type: ASSEMBLY
[id="ocp-4-11-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-11-about-this-release"]
== About this release

// TODO: Update with the relevant information closer to release.
{product-title} (link:https://access.redhat.com/errata/RHSA-2022:TODO[RHSA-2022:TODO]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md[Kubernetes 1.24] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.
//Red Hat did not publicly release {product-title} 4.10.0 as the GA version and, instead, is releasing {product-title} 4.10.3 as the GA version.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. With the {cluster-manager-first} application for {product-title}, you can deploy OpenShift clusters to either on-premises or cloud environments.

// Double check OP system versions
{product-title} {product-version} is supported on {op-system-base-full} 8.4 and 8.5, as well as on {op-system-first} 4.11.

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base} for compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

//TODO: Add the line below for EUS releases.
//{product-title} 4.6 is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

//TODO: The line below is not true for 4.9 but should be used when it is next appropriate. Revisit in October 2022 timeframe.
//With the release of {product-title} 4.9, version 4.6 is now end of life. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].


[id="ocp-4-11-add-on-support-status"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-11-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-11-rhcos"]
=== {op-system-first}

[id="ocp-4-11-nvme-cli"]
==== Improved support for NVMe over Fabrics

{product-title} 4.11 introduces the `nvme-cli` package that provides an interface for managing NVMe devices.

[id="ocp-4-11-kdump-x86"]
==== Investigate kernel crashes on AMD64 machines with kdump

{op-system} now supports `kdump` for the `x86_64` architecture in {product-title} 4.11. Support for `kdump` on other architectures remains in Technology Preview.

[id="ocp-4-11-kdump-arm64"]
==== Investigate kernel crashes on ARM64 machines with kdump (Technology Preview)

{op-system} now supports `kdump` for the `arm64` architecture in {product-title} 4.11 as a Technology Preview.

[id="ocp-4-11-rhcos-rhel-8-6-packages"]
==== {op-system} now uses {op-system-base} 8.6

{op-system} now uses {op-system-base-full} 8.6 packages in {product-title} 4.11 and above. This enables you to have the latest fixes, features, and enhancements, as well as the latest hardware support and driver updates.

[id="ocp-4-11-rhcos-redirector-url"]
==== Updated {op-system} registry URL

The redirector hostname for downloading {op-system} boot images is now `rhcos.mirror.openshift.com`. You must configure your firewall to grant access to the boot images. For more information, see xref:../installing/install_config/configuring-firewall.html#configuring-firewall_configuring-firewall[Configuring your firewall for {product-title}].

[id="ocp-4-11-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-11-rhel-9-openshift-installer"]
==== RHEL 9 support for the OpenShift installer
Using Red Hat Enterprise Linux (RHEL) 9 with the OpenShift installer (openshift-install) is now supported.

For more information, see the "Obtaining the installation program" section of the installation documentation for your platform.

==== New minimum system requirements for installing {product-title} on a single node
This release updates the minimum system requirements for installing {product-title} on a single node. When installing {product-title} on a single node, you should configure a minimum of 16 GB of RAM. Specific workload requirements can require additional RAM. The complete list of supported platforms has been updated to include bare metal, vSphere, {rh-openstack-first}, and Red Hat Virtualization platforms. In all cases, you must specify the `platform.none: {}` parameter in the `install-config.yaml` configuration file when the `openshift-installer` binary is being used to install {sno}.

[id="ocp-4-11-OCP-on-arm"]
==== {product-title} on ARM

{product-title} 4.11 is now supported on ARM architecture based AWS user-provisioned infrastructure and bare-metal installer-provisioned infrastructure. For more information about instance availability and installation documentation, see xref:../installing/installing-preparing.html#supported-installation-methods-for-different-platforms[Supported installation methods for different platforms].

The following features are supported for {product-title} on ARM:

* Disconnected installation support
* Elastic file system (EFS) for AWS
* Local storage operator on bare metal
* Internet Small Computer Systems Interface (iSCSI) for bare metal

The following Operators are supported for {product-title} on ARM:

* Special resource operator (SRO)

[id="ocp-4-11-aws-serial-console-logs"]
==== Troubleshooting bootstrap failures during installation on AWS
The installation program now gathers serial console logs from the bootstrap and control plane hosts on AWS. This log data is added to the standard bootstrap log bundle.

For more information, see xref:../installing/installing-troubleshooting.adoc#installation-bootstrap-gather_installing-troubleshooting[Troubleshooting installation issues].

[id="ocp-4-11-installation-and-upgrade-hyperv"]
==== Support for Microsoft Hyper-V generation version 2
By default, the installation program now deploys a Microsoft Azure cluster using Hyper-V generation version 2 virtual machines (VMs). If the installation program detects that the instance type selected for the VMs does not support version 2, it uses version 1 for the deployment.

[id="ocp-4-11-worker-nodes"]
==== Default AWS and VMware vSphere compute node resources
Beginning with {product-title} 4.11, by default, the installation program now deploys AWS and VMware vSphere compute nodes with 4 vCPUs and 16 GB of virtual RAM.

[id="ocp-4-11-aws-sc2s"]
==== Support for the AWS SC2S region
{product-title} 4.11 introduces support for the AWS Secret Commercial Cloud Services (SC2S) region. You can now install and update {product-title} clusters in the `us-isob-east-1` SC2S region.

For more information, see xref:../installing/installing_aws/installing-aws-secret-region.adoc#installing-aws-secret-region[Installing a cluster on AWS into a Secret or Top Secret Region]

[id="ocp-4-11-nutanix"]
==== Installing a cluster on Nutanix using installer-provisioned infrastructure
{product-title} 4.11 introduces support for installing a cluster on Nutanix using installer-provisioned infrastructure. This type of installation lets you use the installation program to deploy a cluster on infrastructure that the installation program provisions and the cluster maintains.

For more information, see xref:../installing/installing_nutanix/installing-nutanix-installer-provisioned.adoc#installing-nutanix-installer-provisioned[Installing a cluster on Nutanix].

[id="ocp-4-11-azure-ultrassd"]
==== Installing {product-title} using Azure Ultra SSD
You can now enable Ultra SSD storage when installing {product-title} on Azure. This feature requires that both the Azure region and zone where you install {product-title} offer Ultra storage.

For more information, see xref:../installing/installing_azure/installing-azure-customizations.html#installation-configuration-parameters-additional-azure_installing-azure-customizations[Additional Azure configuration parameters].

[id="ocp-4-11-bootstrapExternalStaticIP_{context}"]
==== Added support for bootstrapExternalStaticIP and bootstrapExternalStaticGateway configuration settings

When deploying an installer-provisioned {product-title} cluster on bare metal with static IP addresses and no DHCP server on the `baremetal` network, you must specify a static IP address for the bootstrap VM and the static IP address of the gateway for the bootstrap VM. {product-title} {product-version} provides the `bootstrapExternalStaticIP` and the `bootstrapExternalStaticGateway` configuration settings, which you can set in the `install-config.yaml` file before deployment. The introduction of these settings replaces the workaround procedure link:https://access.redhat.com/articles/6850661[Assigning a bootstrap VM an IP address on the baremetal network without a DHCP server] from the {product-title} 4.10 release.

For more information, see xref:../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#configuring-the-install-config-file_ipi-install-installation-workflow[Configuring the install-config.yaml file] and xref:../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#additional-install-config-parameters_ipi-install-installation-workflow[Additional install-config parameters].

[id="ocp-4-11-configuring-bios-raid-on-fujitsu-Hardware_{context}"]
==== Configuring Fujitsu hardware
{product-title} {product-version} introduces support for configuring the BIOS and RAID arrays of control plane nodes when installing {product-title} on bare metal with Fujitsu hardware. In {product-title} 4.10, configuring the BIOS and RAID arrays on Fujitsu hardware was limited to worker nodes.

For more information, see xref:../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#configuring-the-bios_ipi-install-installation-workflow[Configuring the BIOS] and xref:../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#configuring-the-raid_ipi-install-installation-workflow[Configuring the RAID].

[id="ocp-4-11-oc-mirror-ga"]
==== Disconnected mirroring with the oc-mirror CLI plug-in is now generally available

You can use the oc-mirror OpenShift CLI (`oc`) plug-in to mirror images in a disconnected environment. This feature was previously introduced as a Technology Preview in {product-title} 4.10 and is now generally available in {product-title} 4.11.

This release of the oc-mirror plug-in includes the following new features:

* Pruning images from the target mirror registry
* Specifying version ranges for Operator packages and {product-title} releases
* Generating supporting artifacts for OpenShift Update Service (OSUS) usage
* Obtaining a template for the initial image set configuration

[IMPORTANT]
====
If you used the Technology Preview version of the oc-mirror plug-in for {product-title} 4.10, it is not possible to migrate your mirror registry to {product-title} 4.11. You must download the new oc-mirror plug-in, use a new storage back end, and use a new top-level namespace on the target mirror registry.
====

For more information, see xref:../installing/disconnected_install/installing-mirroring-disconnected.adoc#installing-mirroring-disconnected[Mirroring images for a disconnected installation using the oc-mirror plug-in].

[id="ocp-4-11-azure-encryption"]
==== Installing a cluster on Azure using user-managed encryption keys
{product-title} 4.11 introduces support for installing a cluster on Azure with user-managed disk encryption.

For more information, see xref:../installing/installing_azure/enabling-user-managed-encryption-azure.adoc#enabling-user-managed-encryption-azure[Enabling user-managed encryption for Azure].

[id="ocp-4-11-azure-network-accel"]
==== Accelerated Networking for Azure enabled by default
{product-title} 4.11 on Azure provides accelerated networking for control plane and compute nodes. Accelerated networking is enabled by default for supported instance types in an installer-provisioned infrastructure installation.

For more information, see link:https://access.redhat.com/solutions/6007341[Openshift 4 on Azure - accelerated networking].

[id="ocp-4-11-aws-vpc-endpoints"]
==== AWS VPC endpoints and restricted installations
You are no longer required to configure AWS VPC endpoints when installing a restricted {product-title} cluster on AWS. While configuring VPC endpoints remains an option, you can also choose to configure a proxy without VPC endpoints or configure a proxy with VPC endpoints.

For more information, see xref:../installing/installing_aws/installing-restricted-networks-aws-installer-provisioned.adoc#installation-custom-aws-vpc-requirements_installing-restricted-networks-aws-installer-provisioned[Requirements for using your VPC].

[id="ocp-4-11-capabilities"]
==== Additional customization when installing {product-title}
{product-title} 4.11 allows you to disable the installation of the `baremetal` and `marketplace` Operators, and the `openshift-samples` content that is stored in the `openshift` namespace. You can disable these features by adding the `baselineCapabilitySet` and `additionalEnabledCapabilities` parameters to the `install-config.yaml` configuration file prior to installation. If you disable any of these capabilities during the installation, you can enable them after the cluster is installed. After a capability has been enabled, it cannot be disabled again.

For more information, see the "Installation configuration parameters" section of the installation documentation for your platform.

[id="ocp-4-11-azure-marketplace"]
==== Azure Marketplace offering
{product-title} is now available on the Azure Marketplace. The Azure Marketplace offering is available to customers who procure {product-title} in North America and EMEA.

For more information, see link:https://access.redhat.com/articles/6484031[Installing OpenShift using Azure Marketplace].

[id="ocp-4-11-aws-marketplace"]
==== AWS Marketplace offering
{product-title} is now available on the AWS Marketplace. The AWS Marketplace offering is available to customers who procure {product-title} in North America.

For more information, see link:https://access.redhat.com/articles/6675791[Installing OpenShift using AWS Marketplace].

[id="ocp-4-11-post-installation"]
=== Post-installation configuration

[id="ocp-4-11-cluster-capabilities"]
==== Cluster capabilities
As a cluster administrator, you can enable cluster capabilities to select or deselect one or more optional components before installation or post installation.

For more information, see xref:../post_installation_configuration/cluster-capabilities.adoc#cluster-capabilities[Cluster capabilities].

[id="ocp-4-11-web-console"]
=== Web console

[id="ocp-4-111-developer-perspective"]
==== Developer Perspective

* With this update, in the developer perspective, you can add your GitHub repository containing pipelines to the Openshift Container Platform cluster. You can now run pipelines and tasks from your GitHub repository on the cluster when relevant Git events, such as push or pull requests are triggered.

** In the administrator perspective, you can configure your GitHub application with the OpenShift cluster to use a pipeline as code. With this configuration, you can execute a set of tasks required for build deployment.

* With this update, you can create a customized pipeline using your own set of curated tasks. You can search, install, and upgrade your tasks directly from the developer console.

* With this update, in the web terminal you can now have multiple tabs, view bash history, and the web terminal remains open until you close the browser window or tab.

* With this update, in the *Add+* page of the developer perspective, a new menu added to share project and Helm Chart repositories that allows to add or remove users to the project.

[id="ocp-4-11-dynamic-plug-in-updates"]
==== Dynamic plug-in updates

With this update, you can use the new `console.openshift.io/use-i18next` annotation to determine if the `ConsolePlugin` contains localization resources. If the annotation is set to `"true"`, the localization resources from the i18n namespace named after the dynamic plug-in, are loaded. If the annotation is set to any other value or is missing on the `ConsolePlugin` resource, localization resources are not loaded.

For more information, see xref:../web_console/dynamic-plug-ins.adoc#dynamic-plug-ins_web_console[Dynamic plug-ins].

[id="ocp-4-11-dark-mode"]
==== Support for dark mode theme

The {product-title} web console now supports the dark mode theme. On the *User Preferences* page, select your preferred theme to view the web console in.

[id="ocp-globally-installed-operators-display-operand-instances"]
==== Display operand instances for all managed namespaces on the *Installed Operator* page

With this update, the *Operator* -> *Installed Operator* page will show all Operators across all namespaces. You are still able to view only the instances in the selected namespace within the project selector. When viewing the operand instances, a new switching control allows all operand instances from either all namespaces or only the current namespace to be seen.

[id="ocp-4-11-conditional-updates"]
==== Conditional updates

With this update, if conditional updates are available, you can enable `Include supported but not recommended versions` in the `Select new version` dropdown of the *Update cluster* modal to populate the dropdown list with conditional updates. If a `Supported but not recommended` version is selected, an alert will appear below the dropdown menu displaying potential issues with the version.

[id="ocp-4-11-pod-disruption-budgets"]
==== Pod disruption budgets (PDBs)

This update provides support for pod disruption budgets (PDBs) to the {product-title} web console. From *Workloads* -> *PodDisruptionBudgets*, you can create PDBs for pod resources. You can select `maxUnavailable` and `minAvailable` from the availability requirement list and set the value of pods running. Alternatively, pod disruption budgets can be created from `pod controller resources` list and *Detail* pages. For example, from *Workloads* -> *Deployments* click `Add PodDisruptionBudget`.

For more information, see xref:../nodes/pods/nodes-pods-priority.adoc#priority-preemption-other_nodes-pods-priority[Pod preemption and other scheduler settings].

[id="ocp-4-11-oc"]
=== OpenShift CLI (oc)

[id="ocp-4-11-oc-rhel-9"]
==== {op-system-base} 9 support for the OpenShift CLI (oc)

Using {op-system-base-full} 9 with the OpenShift CLI (`oc`) is now supported.

[NOTE]
====
It is not supported to install the OpenShift CLI (`oc`) as an RPM for {op-system-base-full} 9. You must install the OpenShift CLI for {op-system-base} 9 by downloading the binary.
====

For more information, see xref:../cli_reference/openshift_cli/getting-started-cli.adoc#installing-openshift-cli[Installing the OpenShift CLI].

[id="ocp-4-11-ibm-z"]
=== IBM Z and LinuxONE

With this release, IBM Z and LinuxONE are now compatible with {product-title} {product-version}. The installation can be performed with z/VM or {op-system-base} KVM. For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_z/installing-ibm-z.adoc#installing-ibm-z[Installing a cluster with z/VM on IBM Z and LinuxONE]
* xref:../installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc#installing-restricted-networks-ibm-z[Installing a cluster with z/VM on IBM Z and LinuxONE in a restricted network]
* xref:../installing/installing_ibm_z/installing-ibm-z-kvm.adoc#installing-ibm-z-kvm[Installing a cluster with {op-system-base} KVM on IBM Z and LinuxONE]
* xref:../installing/installing_ibm_z/installing-restricted-networks-ibm-z-kvm.adoc#installing-restricted-networks-ibm-z-kvm[Installing a cluster with RHEL KVM on IBM Z and LinuxONE in a restricted network]

[discrete]
==== Notable enhancements

The following new features are supported on IBM Z and LinuxONE with {product-title} {product-version}:

* Alternate Authentication Provider
* Automatic Device Discovery with Local Storage Operator
* CSI Volumes
** Cloning
** Expansion
** Snapshot
* File Integrity Operator
* Monitoring for user-defined projects
* Operator API
* OC CLI plug-in

[discrete]
==== Supported features

The following features are also supported on IBM Z and LinuxONE:

* Currently, the following Operators are supported:
** Cluster Logging Operator
** Compliance Operator
** Local Storage Operator
** NFD Operator
** NMState Operator
** OpenShift Elasticsearch Operator
** Service Binding Operator
** Vertical Pod Autoscaler Operator
* The following Multus CNI plug-ins are supported:
** Bridge
** Host-device
** IPAM
** IPVLAN
* Encrypting data stored in etcd
* Helm
* Horizontal pod autoscaling
* Multipathing
* Persistent storage using iSCSI
* Persistent storage using local volumes (Local Storage Operator)
* Persistent storage using hostPath
* Persistent storage using Fibre Channel
* Persistent storage using Raw Block
* OVN-Kubernetes, including IPsec encryption
* Support for multiple network interfaces
* Three-node cluster support
* z/VM Emulated FBA devices on SCSI disks
* 4K FCP block device

These features are available only for {product-title} on IBM Z and LinuxONE for {product-version}:

* HyperPAV enabled on IBM Z and LinuxONE for the virtual machines for FICON attached ECKD storage

[discrete]
==== Restrictions

The following restrictions impact {product-title} on IBM Z and LinuxONE:

* The following {product-title} Technology Preview features are unsupported:
** Precision Time Protocol (PTP) hardware

* The following {product-title} features are unsupported:
* Automatic repair of damaged machines with machine health checking
* CodeReady Containers (CRC)
* Controlling overcommit and managing container density on nodes
* FIPS cryptography
* NVMe
* OpenShift Metering
* OpenShift Virtualization
* Tang mode disk encryption during {product-title} deployment

* Compute nodes must run {op-system-first}
* Persistent shared storage must be provisioned by using either {rh-storage-first} or other supported storage protocols
* Persistent non-shared storage must be provisioned using local storage, like iSCSI, FC, or using LSO with DASD, FCP, or EDEV/FBA

[id="ocp-4-11-ibm-power"]
=== IBM Power

With this release, IBM Power is now compatible with {product-title} {product-version}. For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_power/installing-ibm-power.adoc#installing-ibm-power_installing-ibm-power[Installing a cluster on IBM Power]
* xref:../installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc#installing-restricted-networks-ibm-power_installing-restricted-networks-ibm-power[Installing a cluster on IBM Power in a restricted network]

[discrete]
==== Notable enhancements

The following new features are supported on IBM Power with {product-title} {product-version}:

* Alternate Authentication Provider
* CSI Volumes
** Cloning
** Expansion
** Snapshot
* File Integrity Operator
* IPv6
* Monitoring for user-defined projects
* Operator API
* OC CLI plug-in

[discrete]
==== Supported features

The following features are also supported on IBM Power:

* Currently, the following Operators are supported:
** Cluster Logging Operator
** Compliance Operator
** Local Storage Operator
** NFD Operator
** NMState Operator
** OpenShift Elasticsearch Operator
** SR-IOV Network Operator
** Service Binding Operator
** Vertical Pod Autoscaler Operator
* The following Multus CNI plug-ins are supported:
** Bridge
** Host-device
** IPAM
** IPVLAN
* Encrypting data stored in etcd
* Helm
* Horizontal pod autoscaling
* Multipathing
* Multus SR-IOV
* OVN-Kubernetes, including IPsec encryption
* Persistent storage using iSCSI
* Persistent storage using local volumes (Local Storage Operator)
* Persistent storage using hostPath
* Persistent storage using Fibre Channel
* Persistent storage using Raw Block
* Support for multiple network interfaces
* Support for Power10
* Three-node cluster support
* 4K Disk Support

[discrete]
==== Restrictions

The following restrictions impact {product-title} on IBM Power:

* The following {product-title} Technology Preview features are unsupported:
** Precision Time Protocol (PTP) hardware

* The following {product-title} features are unsupported:
** Automatic repair of damaged machines with machine health checking
** CodeReady Containers (CRC)
** Controlling overcommit and managing container density on nodes
** FIPS cryptography
** OpenShift Metering
** OpenShift Virtualization
** Tang mode disk encryption during {product-title} deployment

* Compute nodes must run {op-system-first}
* Persistent storage must be of the Filesystem type that uses local volumes, {rh-storage-first}, Network File System (NFS), or Container Storage Interface (CSI)

[id="ocp-4-11-security"]
=== Security and compliance

[id="ocp-4-11-security-oauth-server-audit-log"]
==== Audit logs now include OAuth server audit events

OAuth server audit events, annotated with login events, are now logged at the metadata level in the audit logs. The login events include failed login attempts.

For more information, see xref:../security/audit-log-policy-config.adoc#about-audit-log-profiles_audit-log-policy-config[About audit log policy profiles].

[id="ocp-4-11-networking"]
=== Networking

==== New option for Ingress Controllers with the hostnetwork endpoint

This update introduces a new option for Ingress Controllers with the `hostnetwork` endpoint strategy. You can now host multiple Ingress Controllers on the same worker node using `httpPort`, `httpsPort`, and `statsPort` binding ports.

[id=ocp-4-11-multiple-configuration-control-worker-nodes]
==== Multi-node configuration for control plane and worker nodes
You can simultaneously apply a single configuration to multiple, bare-metal, installer-provisioned infrastructure nodes in a cluster. Applying a single configuration to multiple nodes reduces the risk of misconfiguration due to the single-provisioning process.

This mechanism is only available for initial deployments when the `install-config` file is used.

[id="ocp-4-11-ne-clb-timeouts"]
==== Support for configuring Classic Load Balancer Timeouts on AWS
You can now configure idle connection timeouts for AWS Classic Load Balancers (CLBs) in the Ingress Controller.

For more information, see xref:../networking/configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-aws.adoc#nw-configuring-clb-timeouts_configuring-ingress-cluster-traffic-aws[Configuring Classic Load Balancer timeouts]

[id="ocp-4-11-ne-tuningoptions-maxconnections"]
==== Support for configuring maximum number of connections for HAProxy processes
You can now set the maximum number of simultaneous connections that can be established per HAProxy process in the Ingress Controller to any value between 2000 and 2,000,000.

For more information, see xref:../networking/ingress-operator.adoc#nw-ingress-controller-configuration-parameters_configuring-ingress[Ingress Controller configuration parameters].

[id="ocp-4-11-configuring-interface-level-safe-network-sysclts"]
==== Support for configuring interface-level safe network sysctls
Use the new `tuning-cni` meta plug-in to set an interface level safe network sysctls that only applies to a specific interface. For example, you can change the behavior of `accept_redirects` on a particular network interface by configuring the `tuning-cni` plug-in. A complete list of the interface-specific safe sysclts that can be set is available in the documentation.

In addition to this enhancement the set of system-wide safe sysctls that can be set has increased to support `net.ipv4.ping_group_range` and `net.ipv4.ip_unprivileged_port_start`.

For more information about configuring the `tuning-cni` plugin, see xref:../networking/setting-interface-level-network-sysctls.adoc#nodes-setting-interface-level-network-sysctls[Setting interface level network sysctls].

For more information about the newly supported interface level network safe sysclts and updates to the list of supported system-wide safe sysclts, see xref:../nodes/containers/nodes-containers-sysctls.adoc#nodes-containers-sysctls[Using sysctls in containers].

[id="ocp-4-11-coredns-forwarding-DNS-requests-over-TLS"]
==== Support for CoreDNS forwarding DNS requests over TLS

When working in a highly regulated environment, you might need the ability to secure domain name system (DNS) traffic when forwarding requests to upstream resolvers so that you can ensure additional DNS traffic and data privacy. Cluster administrators can now configure transport layer security (TLS) for forwarded DNS queries. This feature applies only to the DNS Operator and not the CoreDNS instance managed by the Machine Config Operator.

For more information, see xref:../networking/dns-operator.adoc#nw-dns-forward_dns-operator[Using DNS forwarding].

[id="ocp-4-11-ovn-internal-traffic-policy-support"]
==== Internal traffic support for OVN-Kubernetes

As a cluster administrator, you can configure `internalTrafficPolicy=Local` on a Kubernetes service object when using the OVN-Kubernetes Container Network Interface (CNI) cluster network provider. This feature allows cluster administrators to route traffic to an endpoint on the same node that the traffic originated from. If there are no local node endpoints, traffic will be dropped.

For more information, see link:https://kubernetes.io/docs/concepts/services-networking/service-traffic-policy[Service Internal Traffic Policy].

[id="ocp-4-11-alb-operator-support"]
==== Support for AWS Load Balancer Operator (Technology Preview)

As a cluster administrator, you can install the AWS Load Balancer Operator from the OperatorHub by using the {product-title} web console or CLI. The AWS Load Balancer Operator is in Technology Preview.

For more information, see  xref:../networking/aws_load_balancer_operator/install-aws-load-balancer-operator.adoc#aws-load-balancer-operator[Installing AWS Load Balancer Operator].

[id="ocp-4-11-routes-spec-subdomain-field"]
==== Enhancement to the Route API

Previously, you could not specify the subdomain of a route, and the `spec.host` field was required to set the host name. You can now specify the `spec.subdomain` field and omit the `spec.host` field of a route. The router deployment that exposes the route will use the `spec.subdomain` value to determine the host name.

You can use this enhancement to simplify sharding by enabling a route to have multiple, distinct host names determined by each router deployment that exposes the route.

[id="ocp-4-11-external-dns-operator"]
==== External DNS Operator

In {product-title} 4.11, the External DNS Operator is available for AWS Route53, Azure DNS, GCP DNS, and Infoblox in General Availability (GA) status. The External DNS Operator is still in Technology Preview (TP) status for BlueCat and AWS Route53 on GovCloud.
With this update, the External DNS Operator provides the following enhancements:

* You can create DNS records on a DNS zone on Infoblox.
* By default, the External DNS Operator creates the operands in the namespace `external-dns-operator`. You do not have to manually create a namespace for the operands and Role-based access control (RBAC) prior to installation.
* You can use the status of the route to retrieve the DNS FQDN names.
* The proxy support for BlueCat DNS provider is now available.
* You can enable the automatic DNS configuration deployment while using the BlueCat DNS provider.

Ensure that you migrate from TP to GA. The upstream version of `ExternalDNS` for an {product-title} 4.11 is `v0.12.0` and for the TP is `v0.10.2`.
For more information, see xref:../networking/external_dns_operator/nw-installing-external-dns-operator-on-cloud-providers.adoc#nw-installing-external-dns-operator-on-cloud-providers[About the External DNS Operator].

[id="ocp-4-11-networking-linuxptp-dual-nic"]
==== PTP support for dual NIC boundary clocks

You can now configure dual network interfaces (NICs) as boundary clocks with `PtpConfig` profiles for each NIC channel.

For more information, see xref:../networking/using-ptp.adoc#ptp-dual-nics_using-ptp[Using PTP with dual NIC hardware].

[id="ocp-4-11-networking-linuxptp-events"]
==== PTP events enhancements

A new PTP events API endpoint is available, `api/cloudNotifications/v1/publishers`. Using this endpoint, you can get PTP `os-clock-sync-state`, `ptp-clock-class-change`, and `lock-state` details for the cluster node.

For more information, see xref:../networking/using-ptp.adoc#cnf-fast-event-notifications-api-refererence_using-ptp[Subscribing DU applications to PTP events REST API reference].

[id="ocp-4-11-support-for-pensando-dsc-cards"]
==== SR-IOV Support for Pensando DSC cards

SR-IOV support is now available for xref:../networking/hardware_networks/about-sriov.adoc#supported-devices_about-sriov[Pensando DSC cards]. OpenShift SR-IOV is supported, but you must set a static, Virtual Function (VF) media access control (MAC) address using the SR-IOV CNI config file when using SR-IOV.

[id="ocp-4-11-cidr-ranges-for-network"]
==== {product-title} CIDR Ranges for Networks

Be aware that CIDR ranges for networks are not adjustable after cluster installation. Red Hat does not provide direct guidance on determining the range because it requires careful consideration of the number of pods created.

[id="ocp-4-11-ovn-kubernetes-ipsec-enable"]
==== OVN-Kubernetes network provider: Enable IPsec at runtime

If you are using the OVN-Kubernetes cluster network provider, you can now enable IPsec encryption after cluster installation. For more information about how to enable IPsec, see xref:../networking/ovn_kubernetes_network_provider/configuring-ipsec-ovn.adoc#configuring-ipsec-ovn[Configuring IPsec encryption].

[id="ocp-4-11-additional-metallb-CRDs-logging-control"]
==== Support for additional MetalLB CRDs and control over logging verbosity

Additional MetalLB custom resource defintions (CRDs) have been added to support more complex configurations.

The following CRDs have been added:

* `IPAddressPools`
* `L2Advertisement`
* `BGPAdvertisement`
* `Community`

With these enhancements, you can use the Operator for more complex configurations. For example, you can use the enhancements to isolate nodes or segment the network. In addition, the enhancements made to the FRRouting (FRR) logging component allow you to control the verbosity of the logs generated.

[NOTE]
====
The CRDs documented for {product-title} 4.10 and the existing ways to configure MetalLB as described in link:https://docs.openshift.com/container-platform/4.10/networking/metallb/about-metallb.html[About MetalLB and the MetalLB Operator] are still supported but deprecated. The `AddressPool` configuration is deprecated.

In 4.10, layer 2 and BGP IP addresses using the `AddressPool` were assigned from different address pools. In {product-title} 4.11, layer 2 and BGP IP addresses can be assigned from the same address pool.
====

For more information, see xref:../networking/metallb/about-metallb.adoc#about-metallb[About MetalLB and the MetalLB Operator].

[id="ocp-4-11-create-route-using-destination-ca-certificate"]
==== Ability to create a route using the destination CA certificate in the Ingress annotation

The `route.openshift.io/destination-ca-certificate-secret` annotation can now be used on an Ingress object to define a route with a custom certificate (CA).

See xref:../networking/routes/route-configuration.adoc#nw-ingress-creating-a-route-via-an-ingress_route-configuration[Creating a route using the destination CA certificate in the Ingress annotation] for more information.

[id="ocp-4-11-support-for-hosted-control-planes"]
==== Hosted control planes (Technology Preview)

With hosted control planes for {product-title}, you can host clusters at scale to reduce management costs, optimize cluster deployment time, and separate management and workload concerns. You can enable this deployment model as a Technology Preview feature when you install the multicluster engine for Kubernetes Operator version 2.0. For more information, see xref:../architecture/control-plane.adoc#hosted-control-planes-overview_control-plane[Overview of hosted control planes (Technology Preview)].

Open Virtual Network (OVN) was redesigned to host its control plane and data store alongside the cluster's control plane. With hosted control planes, OVN supports split control planes.

[id="ocp-4-11-dual-stack-user-provisioned-bare-metal-ovn-kubernetes-network-provider"]
==== Dual-stack support on user-provisined bare metal infrastructure with the OVN-Kubernetes cluster network provider

For clusters on user-provisioned xref:../installing/installing_bare_metal/installing-bare-metal-network-customizations.adoc#installation-configuration-parameters-network_installing-bare-metal-network-customizations[bare metal infrastructure], the OVN-Kubernetes cluster network provider supports both IPv4 and IPv6 address families.

[id="ocp-4-11-openshift-on-openstack-hw-offloading"]
==== OVS hardware offloading on {rh-openstack}

For clusters that run on {rh-openstack}, link:https://www.openvswitch.org/[Open vSwitch (OVS)] hardware offloading is now generally available.

For more information, see xref:../post_installation_configuration/network-configuration.adoc#nw-osp-enabling-ovs-offload_post-install-network-configuration[Enabling OVS hardware offloading].

[id="ocp-4-11-openshift-on-openstack-nfv-experience"]
==== NFV user experience improvements on {rh-openstack}

For clusters that run on {rh-openstack}, the network functions virtualization deployment experience is improved. Changes for this release include:

* Network data fetching from metadata service URLs rather than a configuration drive
* Automatic VFIO loading with no-IOMMU for all discovered devices
* DPDK vHost user ports

These changes are reflected in simplified post-installation and network configuration documentation.

[id="ocp-4-11-hardware"]
=== Hardware

[id="ocp-4-11-storage"]
=== Storage

[id="ocp-4-11-registry"]
=== Registry

[id="ocp-4-11-olm"]
=== Operator lifecycle

[id="ocp-4-11-olm-fbc"]
==== File-based catalog format

The default Red Hat-provided Operator catalogs for {product-title} 4.11 releases in the file-based catalog format. {product-title} 4.6 through 4.10 released in the SQLite database format. File-based catalogs are the latest iteration of the catalog format in Operator Lifecycle Manager (OLM). It is a plain text-based file in JSON or YAML and is a declarative configuration evolution of the earlier SQLite database format. Cluster administrators and users will not see any change to their install workflows and Operator consumption with the new catalog format.

For more information, see xref:../operators/understanding/olm-packaging-format.html#olm-file-based-catalogs_olm-packaging-format[File-based catalogs].

[id="ocp-4-11-osdk"]
=== Operator development

[id="ocp-4-11-java-osdk"]
==== Java-based Operators (Technology Preview)

Starting in {product-title} {product-version} as a Technology Preview feature, the Operator SDK includes the tools and libraries to develop a Java-based Operator. Operator developers can take advantage of Java programming language support in the Operator SDK to build a Java-based Operator and manage its lifecycle.

For more information, see xref:../operators/operator_sdk/java/osdk-java-quickstart.adoc[Getting started with Operator SDK for Java-based Operators].

[id="ocp-4-11-run-bundle-fbc-osdk"]
==== Operator SDK support for file-based catalogs

As of {product-title} {product-version}, the `run bundle` command supports the file-based catalog format for Operator catalogs by default. The deprecated SQLite database format for Operator catalogs continues to be supported; however, it will be removed in a future release.

For more information, see xref:../operators/operator_sdk/osdk-working-bundle-images.adoc#osdk-working-bundle-images[Working with bundle images].

[id="ocp-4-11-bundle-validators-osdk"]
==== Validating Operator bundles

As an Operator author, you can run the `bundle validate` command in the Operator SDK to validate the content and format of an Operator bundle. In addition to the default test, you can run optional validators to test for issues in your bundle, such as an empty CRD description or unsupported Operator Lifecycle Manager (OLM) resources.

For more information, see xref:../operators/operator_sdk/osdk-bundle-validate.adoc#osdk-bundle-validate[Validating Operator bundles].

[id="ocp-4-11-builds"]
=== Builds

[id="ocp-4-11-jenkins"]
=== Jenkins

* This enhancement adds a new Jenkins environment variable, `JAVA_FIPS_OPTIONS`, that controls how the JVM operates when running on a FIPS node. For more information, see link:https://access.redhat.com/documentation/en-us/openjdk/11/html-single/configuring_openjdk_11_on_rhel_with_fips/index#config-fips-in-openjdk[OpenJDK support article] (link:https://bugzilla.redhat.com/show_bug.cgi?id=2066019[BZ#2066019])

[id="ocp-4-11-machine-api"]
=== Machine API

[id="ocp-4-11-mapi-aws-imdsv2"]
==== Configuration options for the Amazon EC2 Instance Metadata Service

You can now use machine sets to create compute machines that use a specific version of the Amazon EC2 Instance Metadata Service (IMDS). Machine sets can create compute machines that allow the use of both IMDSv1 and IMDSv2 or compute machines that require the use of IMDSv2.

For more information, see xref:../machine_management/creating_machinesets/creating-machineset-aws.adoc#machineset-imds-options_creating-machineset-aws[Machine set options for the Amazon EC2 Instance Metadata Service].

[id="ocp-4-11-mapi-ultra-disks"]
==== Machine API support for Azure ultra disks

You can now create a machine set running on Azure that deploys machines with ultra disks. You can either deploy machines with ultra disks as data disks, or by using persistent volume claims (PVCs) that use in-tree or Container Storage Interface (CSI) PVCs.

For more information, see the following topics:

* xref:../machine_management/creating_machinesets/creating-machineset-azure.adoc#machineset-azure-ultra-disk_creating-machineset-azure[Machine sets that deploy machines with ultra disks as data disks]

* xref:../storage/container_storage_interface/persistent-storage-csi-azure.adoc#machineset-azure-ultra-disk_persistent-storage-csi-azure[Machine sets that deploy machines with ultra disks using CSI PVCs]

* xref:../storage/persistent_storage/persistent-storage-azure.adoc#machineset-azure-ultra-disk_persistent-storage-azure[Machine sets that deploy machines with ultra disks using in-tree PVCs]

[id="ocp-4-11-mapi-pd-balanced"]
==== Configuration options for Google Cloud Platform persistent disk types

The `pd-balanced` persistent disk type for the Google Cloud Platform (GCP) Compute Engine is now supported. For more information, see xref:../machine_management/creating_machinesets/creating-machineset-gcp.adoc#machineset-gcp-pd-disk-types_creating-machineset-gcp[Configuring persistent disk types by using machine sets].

[id="ocp-4-11-mapi-nutanix-new-platform"]
==== Machine API support for Nutanix clusters

The new platform support for Nutanix clusters includes the ability to manage machines using Machine API machine sets. For more information, see xref:../machine_management/creating_machinesets/creating-machineset-nutanix.adoc[Creating a machine set on Nutanix].

[id="ocp-4-11-capi-tp-aws-gcp"]
==== Managing machines with the Cluster API (Technology Preview)

{product-title} 4.11 introduces the ability to manage machines by using the upstream Cluster API, integrated into {product-title}, as a Technology Preview for AWS and GCP clusters. This capability is in addition or an alternative to managing machines with the Machine API. For more information, see xref:../machine_management/capi-machine-management.adoc[Managing machines with the Cluster API].

[id="ocp-4-11-machine-config-operator"]
=== Machine Config Operator

[id="ocp-4-11-mco-update-zone"]
==== MCO now updates nodes by zone and age

The Machine Config Operator (MCO) now updates the affected nodes alphabetically by zone, based on the `topology.kubernetes.io/zone` label. If a zone has more than one node, the oldest nodes are updated first. For nodes that do not use zones, such as in bare metal deployments, the nodes are upgraded by age, with the oldest nodes updated first. Previously, the MCO did not consider zones or node age.

For more information, see xref:../post_installation_configuration/machine-configuration-tasks.html#machine-config-overview-post-install-machine-configuration-tasks[Machine config overview].

[id="ocp-4-11-mco-enhanced-notification"]
==== Enhanced notification for paused Machine Config Pools upon certificate renewal

You will now receive alerts in the Alerting UI of the {product-title} web console if the MCO attempts to renew an expired `kube-apiserver-to-kubelet-signer` CA certificate on a machine config pool (MCP) that is paused. If the MCPs are paused, the MCO cannot push the newly rotated certificates to those nodes, which can result in failures.

For more information, see xref:../updating/update-using-custom-machine-config-pools.html#update-using-custom-machine-config-pools-pause_update-using-custom-machine-config-pools[Pausing the machine config pools].

[id="ocp-4-11-nodes"]
=== Nodes

[id="ocp-4-11-nodes-self-node-remediation"]
==== Self Node Remediation Operator replaces the Poison Pill Operator
{product-title} 4.11 introduces the Self Node Remediation Operator that replaces the Poison Pill Operator.

The Self Node Remediation Operator provides the following enhancements:

* Introduces separate remediation templates based on the remediation strategy.
* Captures last error message if remediation is unsuccessful.
* Improves metrics for Self Node Remediation Operator's configuration parameters by providing minimum values for the parameters.

For more information, see xref:../nodes/nodes/eco-self-node-remediation-operator.adoc#self-node-remediation-operator-remediate-nodes[Remediating nodes with the Self Node Remediation Operator].

[id="ocp-4-11-nodes-worker-nodes"]
==== Worker nodes for {sno} clusters

You can now add worker nodes to {sno} clusters. This is useful for deployments in resource-constrained environments or at the network edge when you need to add additional capacity to your cluster.

For more information, see xref:../nodes/nodes/nodes-sno-worker-nodes.adoc#nodes-sno-worker-nodes[Worker nodes for {sno} clusters].

[id="ocp-4-11-nodes-descheduler-simulate"]
==== Descheduler now defaults to simulating pod evictions

By default, the descheduler now runs in predictive mode, which means that it only simulates pod evictions. You can review the descheduler metrics to view details about pods that would be evicted.

To evict pods instead of simulating the evictions, change the descheduler mode to automatic.

For more information, see xref:../nodes/scheduling/nodes-descheduler.adoc#nodes-descheduler[Evicting pods using the descheduler].

[id="ocp-4-11-nodes-descheduler-customizations"]
==== New descheduler customizations

This release introduces the following customizations for the descheduler:

* Priority threshold filtering: Set the priority threshold either by class name (`thresholdPriorityClassName`) or by numeric value (`thresholdPriority`) to not evict pods that have a priority that is equal to or greater than that value.

* Namespace filtering: Set a list of user-created namespaces to include or exclude from descheduler operations. Note that protected namespaces (`openshift-*`, `kube-system`, `hypershift`) are always excluded.

* Thresholds for the `LowNodeUtilization` strategy: Set experimental thresholds for underutilization and overutilization for the `LowNodeUtilization` strategy.

For more information, see xref:../nodes/scheduling/nodes-descheduler.adoc#nodes-descheduler[Evicting pods using the descheduler].

[id="ocp-4-11-nodes-node-maintenance-operator"]
==== Node Maintenance Operator enhancements
The Node Maintenance Operator provides the following enhancements:

* Additional feedback, `drainProgress` and `lastUpdate`, is now provided regarding the status of `NodeMaintenance` CR tasks.
* For clusters with bare-metal nodes, an easier method is now available on the web console where you can place a node into maintenance mode, and resume a node from maintenance mode.

For more information, see xref:../nodes/nodes/eco-node-maintenance-operator.adoc#node-maintenance-operator[Using the Node Maintenance Operator to place nodes in maintenance mode].

[id="ocp-4-11-logging"]
=== Logging

[id="ocp-4-11-ocp-on-rhv-logging"]
==== Red Hat OpenShift on {rh-virtualization} Logging (Technology Preview)
{product-title} 4.11 introduces  a new connector for the {rh-virtualization} API that adds automated log messages for all installations and oVirt components in a cluster.

[id="ocp-4-11-monitoring"]
=== Monitoring

The monitoring stack for this release includes the following new and modified features.

[id=ocp-4-11-monitoring-updates-to-monitoring-stack-components-and-dependencies]
==== Updates to monitoring stack components and dependencies

Updates to versions of monitoring stack components and dependencies include the following:

* Alertmanager to 0.24.0
* kube-state-metrics to 2.5.0
* Prometheus to 2.36.2
* Prometheus operator to 0.57.0
* Thanos to 0.26.0

[id=ocp-4-11-monitoring-changes-to-alerting-rules]
==== Changes to alerting rules

[NOTE]
====
Red Hat does not guarantee backward compatibility for recording rules or alerting rules.
====

* *New*
** Added the `KubePersistentVolumeInodesFillingUp` alert, which functions similarly to the existing `KubePersistentVolumeFillingUp` alert but is applied to inodes rather than volume space.
** Added the `PrometheusScrapeBodySizeLimitHit` alert to detect targets hitting the body size limit.
** Added the `PrometheusScrapeSampleLimitHit` alert to detect targets hitting the sample limit.

* *Changed*
** Fixed the `KubeDaemonSetRolloutStuck` alert to use the updated metric `kube_daemonset_status_updated_number_scheduled` from `kube-state-metrics`.
** Replaced the `KubeJobCompletion` alert with `KubeJobNotCompleted`. The new `KubeJobNotCompleted` alert avoids false positives when an earlier job failed but the most recent job succeeded.
** Updated the `NodeNetworkInterfaceFlapping` alert to exclude `tunbr` interfaces from the alert expression.

[id=ocp-4-11-monitoring-enable-alert-routing-for-user-workload-momitoring]
==== Enable alert routing for user workload monitoring
A cluster administrator can now enable alert routing for user workload monitoring so that developers and other users can configure custom alerts and alert routing for their user-defined projects.

[id=ocp-4-11-monitoring-enable-dedicated-alertmanager-for-user-defined-alerts]
==== Enable a dedicated Alertmanager instance for user-defined alerts
You now have the option to enable a separate instance of Alertmanager dedicated to sending alerts only for user-defined projects.
This feature can help reduce the load on the default platform Alertmanager instance and can better separate user-defined alerts from default platform alerts.

[id=ocp-4-11-monitoring-use-additional-authenticaion-settings-for-remote-write]
==== Use additional authentication settings for remote write configuration
You can now use the following authentication methods to access a remote write endpoint: AWS Signature Version 4, custom Authorization header, and OAuth 2.0.
Before this release, you could only use TLS client and basic authentication.

[id=ocp-4-11-monitoring-better-manage-promql-queries-in-web-console]
==== Create, browse, and manage PromQL queries more easily in the web console
The Query Browser on the *Observe* -> *Metrics* page of the {product-title} web console adds various enhancements to improve your ability to create, browse, and manage PromQL queries.
For example, administrators can now duplicate existing queries and use autocomplete suggestions when building and editing queries.

[id=ocp-4-11-monitoring-doubled-scrape-interval-for-servicemonitors-single-node]
====  Scrape interval doubled for ServiceMonitors in single node deployments
The scrape interval has been doubled for all Cluster Monitoring Operator (CMO) controlled ServiceMonitors on single-node {product-title} deployments.
The maximum interval is now two minutes.

[id=ocp-4-11-monitoring-create-alerting-rules-based-on-platform-metrics]
==== Create alerting rules based on platform monitoring metrics (Technology Preview)
This release introduces a Technology Preview feature in which administrators can create alerting rules based on existing platform monitoring metrics.
This feature helps administrators to more quickly and easily create new alerting rules specific to their environments.

[id=ocp-4-11-monitoring-add-cluster-id-labels-for-remote-write]
==== Add cluster ID labels to remote write storage
You can now add cluster ID labels to metrics being sent to remote write storage.
You can then query these labels to identify the source cluster for a metric and distinguish that metric data from similar metric data sent by other clusters.

[id=ocp-4-11-monitoring-query-metrics-using-federation-endpoint]
==== Query metrics using the federation endpoint for user workload monitoring
You can now use the Prometheus `/federate` endpoint to scrape user-defined metrics from a network location outside the cluster.
Before this release, you could only access the federation endpoint to scrape metrics in default platform monitoring.

[id=ocp-4-11-monitoring-enable-body-size-limit-metrics-scraping]
==== Enable body size limit for metrics scraping for default platform monitoring
You can now set the `enforcedBodySizeLimit` config map option for default platform monitoring to enable a body size limit on metrics scraping.
This setting triggers the new `PrometheusScrapeBodySizeLimitHit` alert when at least one Prometheus scrape target replies with a response body larger than the configured `enforcedBodySizeLimit`.
The setting can limit the impact that a malicious target can have on both the Prometheus component and on the cluster as a whole.

[id=ocp-4-11-monitoring-confgure-retention-size-for-metrics-storage]
==== Configure retention size settings for metrics storage
You can now configure the maximum amount of disk space reserved for retained metrics storage for both default platform monitoring and user-workload monitoring.
Before this release, you could not configure this setting.

[id=ocp-4-11-monitoring-confgure-retention-time-for-thanos-ruler]
==== Configure the retention time period for Thanos Ruler in user-defined projects
You can now configure the retention time period for Thanos Ruler data in user-defined projects.
Before this release, you could not change the default value of `24h`.

[id="ocp-4-11-scalability-and-performance"]
=== Scalability and performance

[id=ocp-4-11-enhance-scaleup-operations-etcd-clusters]
==== Enhancement to scale-up operations for etcd clusters
The Raft algorithm allows for scaling of etcd members using a new `learner` state. As a result, maintaining cluster quorum, adding and removing new members, and promoting `learners` occur without disrupting the cluster operation.

[id=ocp-4-11-enabling-assisted-installer-service-on-bare-metal_{context}]
{product-title} 4.11 introduces the `imageStorage` option as part of the `AgentServiceConfig` custom resource. This option improves application performance by allowing the user to specify persistent storage claim details for use with the image service.

The `releaseImage` parameter of the `ClusterImageSet` custom resource now supports operating system image version identification. The discovery ISO is based on an operating system image version as the releaseImage, or the latest version if the specified version is unavailable.

The `openshiftVersion` parameter of the `AgentServiceConfig` custom resource (CR) now supports either "x.y" (major.minor) or "x.y.z" (major.minor.patch) formats.

[id=ocp-4-11-ne-livenessProbe-readinessProbe-startupProbe-timeout-configuration]
// This is a 4.11 Network Edge feature that is located in the Scalability and Performance area of the docs, requested to be placed here by Miciah Masters -Sara T.
==== Configuring Ingress Controller (router) Liveness, Readiness, and Startup probes
{product-title} 4.11 introduces the ability to configure the timeout values for the kubelet’s liveness, readiness, and startup probes for router deployments that are managed by the OpenShift Container Platform ingress operator. The ability to set larger timeout values can reduce the risk of unnecessary and unwanted restarts that are caused by short default timeout of 1 second.

For more information see, xref:../scalability_and_performance/routing-optimization.adoc#configuring-ingress-controller-router-liveness-readiness-and-startup-probes[Configuring Ingress Controller (router) Liveness, Readiness, and Startup probes]

[id=ocp-4-11-nto-offlined-cpu]
==== New power reduction CPU capability

You can decrease power consumption through the Node Tuning Operator by specifying CPUs in the `offlined` field in the performance profile. For more information, see xref:../scalability_and_performance/cnf-low-latency-tuning.adoc#node-tuning-operator-disabling-CPUs-for-power-consumption_cnf-master[Reducing power consumption by taking CPUs offline].

[id=ocp-4-11-node-observability-operator]
==== Node Observability Operator (Technology Preview)
{product-title} 4.11 introduces the Node Observability Operator in Technology Preview.

The Node Observability Operator provides the ability to:

* Deploy Node Observability Agents on the worker nodes.
* Trigger a CRIO-O and Kubelet profiling.
* Make the profiling data files available for further analysis.

For more information, see xref:../scalability_and_performance/node-observability-operator.adoc#node-observability-operator[Requesting CRI-O and Kubelet profiling data using the Node Observability Operator].

[id="ocp-4-11-PAO-to-NTO"]
==== Performance Addon Operator functions moved to the Node Tuning Operator

In earlier versions of {product-title}, the Performance Addon Operator provided automatic, low latency performance tuning for applications. In {product-title} 4.11, these functions are part of the Node Tuning Operator. The Node Tuning Operator is part of the standard installation for {product-title} 4.11. If you upgrade to {product-title} 4.11, the Node Tuning Operator removes the Performance Addon Operator and all related artifacts on startup.

For more information, see xref:../operators/operator-reference.adoc#about-node-tuning-operator_platform-operators-ref[Node Tuning Operator].

[NOTE]
====
You must still use the `performance-addon-operator-must-gather` image when running the `must-gather` command with the Performance Profile Creator. For more information, see xref:../scalability_and_performance/cnf-create-performance-profiles.adoc#gathering-data-about-your-cluster-using-must-gather_cnf-create-performance-profiles[Gathering data about your cluster using must-gather]
====

[id="ocp-4-11-PAO-to-NTO-docs"]
==== Low latency tuning documentation updates

In earlier version of {product-title}, documentation for low latency tuning included references to the Performance Addon Operator. Since the Node Tuning Operator now provides low latency tuning, the documentation title changed from "Performance Addon Operator for low latency nodes" to "Low latency tuning", and multiple cross-references to this document were updated accordingly. For more information, see xref:../scalability_and_performance/cnf-low-latency-tuning.adoc#cnf-low-latency-tuning[Low latency tuning].

[id="ocp-4-11-SRO-hub-and-spoke"]
==== Hub and spoke cluster support

For hub and spoke deployments in which spoke clusters require out-of-tree driver support, you can use the Special Resource Operator (SRO) deployed in the hub cluster to manage the deployment of the required kernel module(s) to one or more managed clusters. This uses {rh-rhacm-first} and no longer requires the use of Node Feature Discovery (NFD). For more information, see xref:../hardware_enablement/psap-special-resource-operator.adoc#deploy-simple-kmod-using-crd[Building and running the simple-kmod SpecialResource for a hub-and-spoke topology].

[id="ocp-4-11-SRO-upgrade"]
====  Enhanced SRO cluster upgrade support

When upgrading a cluster in which special resources are being managed, you can run the pre-upgrade custom resource to verify that a new driver container exists to support a kernel update. This helps avoid possible disruption of managed special resources. The documentation for this feature is currently unavailable and is targeted for release at a later date.

[id="ocp-4-11-SRO-debugging-and-logging"]
====  Enhanced debugging and logging for SRO

The Special Resource Operator (SRO) includes a consistent log output format with greater message detail for troubleshooting.

[id="ocp-4-11-SRO-external-registries"]
====  Support for external registries

Before this update, SRO did not support connecting to registries in disconnected environments. This release provides support for disconnected environments for which the driver container is hosted in a registry outside of the {product-title} cluster.

[id="ocp-4-11-backup-and-restore"]
=== Backup and restore

[id="ocp-4-11-dev-exp"]
=== Developer experience

[id="ocp-4-11-insights-operator"]
=== Insights Operator

==== Insights Operator data collection enhancements
In {product-title} {product-version}, the Insights Operator collects the following additional information:

* The `images.config.openshift.io` resource definition
* `kube-controller-manager` container logs when the `"Internal error occurred: error resolving resource"` or `"syncing garbage collector with updated resources from discovery"` error messages are present
* `storageclusters.ocs.openshift.io/v1` resources

With this additional information, Red Hat improves {product-title} functionality and enhances Insights Advisor recommendations.

[id="ocp-4-11-auth"]
=== Authentication and authorization

[id="ocp-4-11-auth-oidc-providers"]
==== Additional supported OIDC providers

The following OpenID Connect (OIDC) providers are now tested and supported with {product-title}:

* Active Directory Federation Services for Windows Server
+
[NOTE]
====
Currently, it is not supported to use Active Directory Federation Services for Windows Server with {product-title} when custom claims are used.
====

* Microsoft identity platform (Azure Active Directory v2.0)
+
[NOTE]
====
Currently, it is not supported to use Microsoft identity platform when group names are required to be synced.
====

For the full list of OIDC providers, see xref:../authentication/identity_providers/configuring-oidc-identity-provider.adoc#identity-provider-oidc-supported_configuring-oidc-identity-provider[Supported OIDC providers].

[id="ocp-4-11-auth-pod-security-admission"]
==== Pod security admission

link:https://kubernetes.io/docs/concepts/security/pod-security-admission/[Pod security admission] is now enabled on {product-title}.

Pod admission is enforced by both pod security and security context constraints (SCC) admissions. Pod security admission runs globally with `privileged` enforcement and `restricted` audit logging and API warnings.

A controller monitors the xref:../authentication/managing-security-context-constraints.adoc#managing-pod-security-policies[SCC-related permissions] of service accounts in user-created namespaces and automatically labels these namespaces with link:https://kubernetes.io/docs/concepts/security/pod-security-standards/[pod security admission] `warn` and `audit` labels.

To improve workload security in accordance with the `restricted` pod security profile, this release introduces SCCs that enforce pod security in accordance with new pod security admission controls. These SCCs are:

* `restricted-v2`
* `hostnetwork-v2`
* `nonroot-v2`

They correspond to older, similarly named SCCs, but with the following enhancements:

* `ALL` capabilities are dropped from containers. Previously, only the `KILL`, `MKNOD`, `SETUID`, and `SETGID` capabilities were dropped.
* The `NET_BIND_SERVICE` capability can now be added explicitly.
* `seccompProfile` is defaulted to `runtime/default` if unset. In previous releases, this field had to be empty.
* `allowPrivilegeEscalation` must be unset or set to `false` in security contexts. Previously, a `true` value was allowed.

New clusters allow use of the `restricted-v2` SCC for any authenticated user in place of the `restricted` SCC; the `restricted` SCC is no longer available to users of new clusters, unless the access is explicitly granted. Clusters that are upgraded to {product-title} 4.11 from previous versions allow both `restricted-v2` and `restricted` SCCs.

You can enable synchronization for most namespaces and disable synchronization for all namespaces.

For this release, namespaces that are prefixed with `openshift-` do not have restricted enforcement. Restricted enforcement for such namespaces is planned for inclusion in a future release.

For more information, see xref:../authentication/understanding-and-managing-pod-security-admission.adoc#understanding-and-managing-pod-security-admission[Understanding and managing pod security admission].

[id="ocp-4-11-notable-technical-changes"]
== Notable technical changes

{product-title} {product-version} introduces the following notable technical changes.

// Note: use [discrete] for these sub-headings.

[discrete]
[id="ocp-4-11-router-load-balancing-algorithm-default-update"]
==== Update in default value for setting the router load-balancing algorithm

The `haproxy.router.openshift.io/balance` variable, which sets the router load-balancing algorithm, now defaults to the value `random` instead of `leastconn`. See xref:../networking/routes/route-configuration.adoc#nw-route-specific-annotations_route-configuration[Route-specific annotations] for more information.

[discrete]
[id="ocp-4-11-legacy-service-account"]
==== LegacyServiceAccountTokenNoAutoGeneration is on by default

To align with upstream link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#urgent-upgrade-notes-1[Kubernetes having moved] the `LegacyServiceAccountTokenNoAutoGeneration` feature gate to beta and enabling it by default, {product-title} now also follows this security feature and releasess with the feature enabled. As a result, when creating new service accounts (SA), a service account token secret is no longer automatically generated. Previously, {product-title} automatically added a service account token to a secret for each new SA.

If you need a service account token secret, you must manually use the TokenRequest API to request bound service account tokens or create a service account token secret.

After updating to {product-version}, existing service account token secrets are not deleted and continue to function as expected.

Service Account token secrets still appear as auto-generated in {product-title} 4.11. However, instead of two secrets per Service Account, there will now be only one, which will be further reduced to zero in a future release. These tokens do not work. For now, dockercfg secrets are still being generated as secrets and no secrets will be deleted during upgrades.

* For information about using the TokenRequest API, see xref:../authentication/bound-service-account-tokens.html#bound-sa-tokens-configuring_bound-service-account-tokens[Using bound service account tokens]

* For information about creating a service account token secret, see xref:../nodes/pods/nodes-pods-secrets.html#nodes-pods-secrets-creating-sa_nodes-pods-secrets[Creating a service account token secret].

[discrete]
[id="ocp-4-11-operator-sdk-1-22-0"]
==== Operator SDK 1.22.0

{product-title} 4.11 supports Operator SDK 1.22.0. See xref:../cli_reference/osdk/cli-osdk-install.adoc#cli-osdk-install[Installing the Operator SDK CLI] to install or update to this latest version.

[NOTE]
====
Operator SDK 1.22.0 supports Kubernetes 1.24.
====

If you have Operator projects that were previously created or maintained with Operator SDK 1.16.0, update your projects to keep compatibility with Operator SDK 1.22.0.

* xref:../operators/operator_sdk/golang/osdk-golang-updating-projects.adoc#osdk-upgrading-projects_osdk-golang-updating-projects[Updating Go-based Operator projects]

* xref:../operators/operator_sdk/ansible/osdk-ansible-updating-projects.adoc#osdk-upgrading-projects_osdk-ansible-updating-projects[Updating Ansible-based Operator projects]

* xref:../operators/operator_sdk/helm/osdk-helm-updating-projects.adoc#osdk-upgrading-projects_osdk-helm-updating-projects[Updating Helm-based Operator projects]

* xref:../operators/operator_sdk/helm/osdk-hybrid-helm-updating-projects.adoc#osdk-upgrading-projects_osdk-hybrid-helm-updating-projects[Updating Hybrid Helm-based Operator projects]

[id="ocp-4-11-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the table, features are marked with the following statuses:

* *GA*: _General Availability_
* *DEP*: _Deprecated_
* *REM*: _Removed_

//TODO: remove anything that has been REM since 4.7?

.Deprecated and removed features tracker
[cols="3,1,1,1",options="header"]
|====
|Feature |OCP 4.9 |OCP 4.10 | OCP 4.11

|Package manifest format (Operator Framework)
|REM
|REM
|

|SQLite database format for Operator catalogs
|DEP
|DEP
|

|`oc adm catalog build`
|REM
|REM
|REM

|`--filter-by-os` flag for `oc adm catalog mirror`
|REM
|REM
|REM

|v1beta1 CRDs
|REM
|REM
|REM

|Docker Registry v1 API
|REM
|REM
|REM

|Metering Operator
|REM
|REM
|REM

|Scheduler policy
|DEP
|REM
|REM

|`ImageChangesInProgress` condition for Cluster Samples Operator
|DEP
|DEP
|DEP

|`MigrationInProgress` condition for Cluster Samples Operator
|DEP
|DEP
|DEP

|Use of `v1` without a group in `apiVersion` for {product-title} resources
|REM
|REM
|REM

|Use of `dhclient` in {op-system}
|REM
|REM
|REM

|Cluster Loader
|DEP
|REM
|REM

|Bring your own {op-system-base} 7 compute machines
|DEP
|REM
|REM

|`lastTriggeredImageID` field in the `BuildConfig` spec for Builds
|REM
|REM
|REM

|Jenkins Operator
|DEP
|REM
|REM

|HPA custom metrics adapter based on Prometheus
|REM
|REM
|REM

|Grafana component in monitoring stack
|
|DEP
|REM

|Access to Prometheus and Grafana UIs in monitoring stack
|
|DEP
|REM

|vSphere 6.7 Update 2 or earlier
|DEP
|DEP
|

|Virtual hardware version 13
|DEP
|DEP
|

|VMware ESXi 6.7 Update 2 or earlier
|DEP
|DEP
|

|Minting credentials for Microsoft Azure clusters
|GA
|REM
|REM

|Persistent storage using FlexVolume
|
|
|

|Automatic generation of service account token secrets
|GA
|GA
|REM

|Removal of Jenkins images from install payload
|GA
|GA
|REM

|====

[id="ocp-4-11-deprecated-features"]
=== Deprecated features

[id="ocp-4-11-oc-commands-flags-tokens-deprecated"]
==== OpenShift CLI (oc) commands and flags for requesting tokens are deprecated

The following `oc` commands and flags for requesting tokens are now deprecated:

* The `oc serviceaccounts create-kubeconfig` command
* The `oc serviceaccounts get-token` command
* The `oc serviceaccounts new-token` command
* The `--service-account/-z` flag for the `oc registry login` command

Use the `oc create token` command instead to request tokens.

[id="ocp-4-11-rhv-deprecations"]
==== Red Hat Virtualization (RHV) as a host platform for {product-title} will be deprecated

Red Hat Virtualization (RHV) will be deprecated in an upcoming release of {product-title}. Support for {product-title} on RHV will be removed from a future {product-title} release, currently planned as {product-title} 4.14.

[id="ocp-4-11-removed-features"]
=== Removed features

[id="ocp-4-11-oc-rhel-7"]
==== {op-system-base} 7 support for the OpenShift CLI (oc) has been removed

Support for using {op-system-base-full} 7 with the OpenShift CLI (`oc`) has been removed. If you use the OpenShift CLI (`oc`) with {op-system-base}, you must use {op-system-base} 8 or later.

[id="ocp-4-11-oc-commands-removed"]
==== OpenShift CLI (oc) commands have been removed

The following OpenShift CLI (`oc`) commands were removed with this release:

* `oc adm migrate etcd-ttl`
* `oc adm migrate image-references`
* `oc adm migrate legacy-hpa`
* `oc adm migrate storage`

[id="ocp-4-11-monitoring-grafana-component-removed"]
==== Grafana component removed from monitoring stack
The Grafana component is no longer a part of the {product-title} {product-version} monitoring stack.
As an alternative, go to *Observe* -> *Dashboards* in the {product-title} web console to view monitoring dashboards.

[id="ocp-4-11-monitoring-prometheus-and-grafana-uis-removed"]
==== Prometheus and Grafana user interface access removed from monitoring stack
Access to the third-party Prometheus and Grafana user interfaces have been removed from the {product-title} {product-version} monitoring stack.
As an alternative, click *Observe* in the {product-title} web console to view alerting, metrics, dashboards, and metrics targets for monitoring components.

[id="ocp-4-11-custom-scheduler"]
==== Support for manually deploying a custom scheduler has been removed

Support for deploying custom schedulers manually has been removed with this release. Use the xref:../nodes/scheduling/secondary_scheduler/index.adoc#nodes-secondary-scheduler-about_nodes-secondary-scheduler-about[{secondary-scheduler-operator-full}] instead to deploy a custom secondary scheduler in {product-title}.

[id="ocp-4-11-openshiftsdn-sno-not-supported"]
==== Support for deploying {sno} with OpenShiftSDN has been removed

Support for deploying {sno} clusters with OpenShiftSDN has been removed with this release. OVN-Kubernetes is the default networking solution for {sno} deployments.

==== Removal of Jenkins images from install payload

* {product-title} 4.11 moves the "OpenShift Jenkins" and "OpenShift Agent Base" images to the `ocp-tools-4` repository at `registry.redhat.io` so that Red Hat can produce and update the images outside the {product-title} lifecycle. Previously, these images were in the {product-title} install payload and the `openshift4` repository at `registry.redhat.io`. For more information, see xref:../cicd/jenkins/openshift-jenkins.adoc#openshift-jenkins[OpenShift Jenkins].

* {product-title} 4.11 removes "OpenShift Jenkins Maven" and "NodeJS Agent" images from its payload. Previously, {product-title} 4.10 deprecated these images. Red Hat no longer produces these images, and they are not available from the `ocp-tools-4` repository at `registry.redhat.io`.
+
However, upgrading to {product-title} 4.11 does not remove "OpenShift Jenkins Maven" and "NodeJS Agent" images from 4.10 and earlier releases. And Red Hat provides bug fixes and support for these images through the end of the 4.10 release lifecycle, in accordance with the link:https://access.redhat.com/support/policy/updates/openshift[{product-title} lifecycle policy].
+
For more information, see xref:../cicd/jenkins/openshift-jenkins.adoc#openshift-jenkins[OpenShift Jenkins].

[id="ocp-4-11-future-removals"]
=== Future Kubernetes API removals

The next minor release of {product-title} is expected to use Kubernetes 1.25. Currently, Kubernetes 1.25 is scheduled to remove several deprecated `v1beta1` and `v2beta1` APIs.

See the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25[Deprecated API Migration Guide] in the upstream Kubernetes documentation for the list of planned Kubernetes API removals.

See link:https://access.redhat.com/articles/6955985[Navigating Kubernetes API deprecations and removals] for information on how to check your cluster for Kubernetes APIs that are planned for removal.

[id="ocp-4-11-bug-fixes"]
== Bug fixes
//Bug fix work for TELCODOCS-750
[discrete]
[id="ocp-4-11-bare-metal-hardware-bug-fixes"]
==== Bare Metal Hardware Provisioning

* Previously, when writing the RHCOS image onto some disks, the `qemu-img` was allocating space onto the entire disk, including sparse areas. This extended the time of the writing process on some hardware. This update disables the `qemu-img` sparse in image creation. As a result, the image writing would no longer take a long time on affected hardware. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2002009[*BZ#2002009*])

* Previously, if the `rotational` field was set for `RootDeviceHints`, the host could fail the provision. With this update, the `rotational` field in `RootDeviceHints` is properly copied and checked. As a result, the provisioning will succeed when using the `rotational` field. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2053721[*BZ#2053721*])

* Previously, Ironic was unable to use virtual media to provision Nokia OE 20 servers because the BMC required the `TransferProtocolType` attribute to be explicitly set in the request despite this being an optional attribute. Additionally, the BMC also required the use of a dedicated `RedFish` settings resource to override boot orders, whereas most BMCs just use the `system` resource. This error occurred because Nokia OE 20 strictly requires an optional `TransferProtocolType` attribute for vMedia attachments and requires the use of the `RedFish` settings resource for overriding boot sequences. Consequently, virtual media based provisioning would fail on Nokia OE 20. There are two workarounds for this issue:
. When the vMedia attachment request fails with an error indicating that the `TransferProtocolType` attribute is missing, retry the request and explicitly specify this attribute.
. Check for the presence of the RedFish settings resource in the system. If it is present, use it for the boot sequence override.
As a result, virtual media based provisioning will succeed on Nokia OE 20 machines. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2059567[*BZ#2059567*])

* Previously, the Ironic API inspector image failed to clean disks that were part of passive multipath setups when using {product-title} bare metal IPI deployments. This update fixes the failures when active or passive storage arrays are in use. As a result, it is now possible to use {product-title} bare metal IPI when customers want to use multipath setups that are active or passive. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2089309[*BZ#2089309]*)

* Previously, Ironic failed to match `wwn` serial numbers to multi-path devices. Consequently, `wwn` serial numbers for device mapper devices could not be used in the `rootDeviceHint` parameter in the`install-config.yaml` configuration file. With this update, Ironic now recognizes `wwn` serial numbers as unique identifiers for multi-path devices. As a result, it is now possible to use `wwn` serial numbers for device mapper devices for the `install-config.yaml` file. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2098392[*BZ#2098392]*)

[discrete]
[id="ocp-4-11-builds-bug-fixes"]
==== Builds

* Previously, using a forward slash (`/`) in the `ImageLabel` name of a `BuildConfig` instance resulted in an error. This fix resolves the issue by changing the utility used for validation. As a result, you can use a forward slash in the `ImageLabel` name of a `BuildConfig` instance. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2105167[*BZ#2105167*])

* Previously, when using the `$ oc new-app --search <image_stream_name>` command, you could receive an incorrect message related to `docker.io` images. This resulted in confusion for the user because {product-title} should not be using image streams that point to `docker.io`. This fix adds code checks to prevent the reference to `docker.io`. As a result, the output from that command does not include the message. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2049889[*BZ#2049889*])

* Previously, Shared Resource CSI Driver metrics were not exported to the Telemetry service. As a result, usage metrics for the Shared Resource CSI Driver could not be analyzed. With this fix, Shared Resource CSI Driver metrics are exposed to the Telemetry service. As a result, usage metrics for the Shared Resource CSI Driver can be collected and analyzed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2058225[*BZ#2058225*])


* By default, Buildah prints steps to the log file, including the contents of environment variables, which might include xref:../cicd/builds/creating-build-inputs.adoc#builds-input-secrets-configmaps_creating-build-inputs[build input secrets]. Although you can use the `--quiet` build argument to suppress printing of those environment variables, this argument isn't available if you use the source-to-image (S2I) build strategy. The current release fixes this issue. To suppress printing of environment variables, set the `BUILDAH_QUIET` environment variable in your build configuration:
+
[source,yaml]
----
sourceStrategy:
...
  env:
    - name: "BUILDAH_QUIET"
      value: "true"
----

[discrete]
[id="ocp-4-11-cloud-compute-bug-fixes"]
==== Cloud Compute

* `CertificateSigningRequest` (CSR) resource renewal is handled by the Kubernetes controller manager and correctly left pending by the Cluster Machine Approver Operator, which increases the value of the `mapi_current_pending_csr` metric to `1`. Previously, when the  Kubernetes controller manager approved the CSR, the Operator ignored it and left the metric unchanged. As a result, the `mapi_current_pending_csr` metric was stuck at `1` until the Operator next reconciled. With this release, CSR approvals from other controllers are always reconciled to update metrics and the value of the `mapi_current_pending_csr` metric is updated after every reconcile. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2047702[*BZ#2047702*])

* Previously, only regions contained in a list of known regions within the AWS SDK were validated, and specifying any other region caused an error. This meant that, as new regions were added, they could not be used until the SDK was updated to contain the new region information. With this release, regions are validated with a less strict setting that warns the user when a region is not recognized. As a result, new regions might cause warning messages but can be used immediately. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2065510[*BZ#2065510*])

* Previously, the Cluster Machine Approver Operator appended the `"Approved"` status condition to its condition list. As a result, the Kubernetes API server logged errors containing the message `[SHOULD NOT HAPPEN] failed to update managedFields`. With this release, the Operator is updated to check its conditions before appending to the list, and only update the condition when necessary. As a result, the conditions are no longer duplicated in the `CertificateSigningRequest` resource and the Kubernetes API server no longer logs errors about the duplicate. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1978303[*BZ#1978303*])

* Previously, a defect in the Cisco ACI neutron implementation that was present in {rh-openstack-first} version 16, caused the query for subnets belonging to a given network to return unexpected results. As a result, the {rh-openstack} Cluster API provider might try to provision instances with duplicated ports on the same subnet, leading to a failed provisioning. With this release, additional filtering in the {rh-openstack} Cluster API provider ensures there is no more than one port per subnet, and it is now possible to deploy {product-title} on {rh-openstack} version 16 with Cisco ACI. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2033862[*BZ#2033862*])

* Previously, the {rh-openstack-first} Machine API provider did not use the proxy environment variable directives, causing installation behind an HTTP or HTTPS proxy to fail. With this release, the provider obeys proxy directives and functions correctly in a restricted environment in which egress traffic is only allowed through a proxy.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2046133[*BZ#2046133*])

* Previously, when upgrading from {product-title} 4.9 to 4.10, inconsistencies between multiple controllers resulted in incorrect version numbers. As a result, the version number was not consistent. With this release, consistent reading of version numbers has been enacted and the release version is now stable in the cluster operator status. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2059716[*BZ#2059716*])

* Previously, leaks of load balancer targets on AWS Machine API providers may occur. This is because IP-based load balancer attachments may remain within the load balancer registration when replacing control plane machines. With this release, IP-based load balancer attachments are removed from the load balancer before the Amazon EC2 instance is removed from AWS. As a result, the leaks are avoided. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2065160[*BZ#2065160*])

* Previously, during an upgrade, a new machine created through the Machine API defaulted to HW-13 which caused the cluster to degrade. With this release, the machine controller checks the virtual machine’s hardware version during machine creation from the template clone. The machine goes into a `failed` state if the template’s hardware version is less than 15, which is the minimal supported hardware version for {product-title} 4.11 and above versions. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2059338[*BZ#2059338*])

* Previously, the procedural name generator for Azure availability sets exceeded the 80 character maximum limit. This might cause the Machine API to reuse the same sets during the name truncation, rather than creating multiple availability sets. With this release, the procedural name generator is updated to ensure that the name is no longer than 80 characters and the cluster name is not duplicated in the set name. As a result, Azure availability sets are no longer truncated in unexpected ways by the procedural name generator. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2093044[*BZ#2093044*])

* Because the Cluster Autoscaler Operator was deploying the cluster autoscaler without setting any leader election parameters, the cluster autoscaler could unexpectedly fail and restart after a cluster restart. With this fix, the Cluster Autoscaler Operator now deploys the cluster autoscaler with well-defined leader election flags. As a result, the cluster autoscaler operates as expected after restarts. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2063194[*BZ#2063194*])"

* Previously, certificate signing request (CSR) renewal was handled by `kube-controller-manager` and correctly left pending by the machine approver, which increased `mapi_current_pending_csr` to `1`. Afterwards, the `kube-controller-manager` approved the CSR, but the machine approver would ignore it, which left the metric unchanged. Consequently, the `mapi_current_pending_csr` was stuck at `1` until another machine approver reconciled it. With this update, CSR approvals are reconciled from other controllers to update metrics properly. As a result, `mapi_current_pending_csr` is always up-to-date after every reconcile. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2072195[*BZ#2072195*])

[discrete]
[id="ocp-4-11-cloud-credential-operator-bug-fixes"]
==== Cloud Credential Operator

[discrete]
[id="ocp-4-11-cluster-version-operator-bug-fixes"]
==== Cluster Version Operator

[discrete]
[id="ocp-4-11-console-storage-bug-fixes"]
==== Console Storage Plug-in

[discrete]
[id="ocp-4-11-dns-bug-fixes"]
==== Domain Name System (DNS)

[discrete]
[id="ocp-4-11-image-registry-bug-fixes"]
==== Image Registry

* Previously, the image registry used an `ImageContentSourcePolicy` (ICSP) as a source only when it was an exact match. The same source file was expected to be pulled through for any subrepositories. The ICSP name and path did not match for subrepositories. As a result, the image was not used. Now, the ICSP is applied successfully to the subrepositories, which can use the mirrored images. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2014240[*BZ#2014240*])

[discrete]
[id="ocp-4-11-image-streams-bug-fixes"]
==== Image Streams

[discrete]
[id="ocp-4-11-installer-bug-fixes"]
==== Installer

* Previously, if users specified an {product-title} cluster name with a period, the installation failed. This update adds a validation check to the installation program, which returns an error if a period is present in the cluster name. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2084580[*BZ#2084580*])

* Previously, users could select the AWS `us-gov-east-1` region when using the installation program to create the `install-config.yaml` file. This caused the deployment to fail because the installation program could only be used to create the `install-config.yaml` file for a public AWS region. This update removes all of the AWS regions from the installation program that are not supported by the public AWS cloud. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2048222[*BZ#2048222*])

* Previously, users could not select the `ap-north-east-3` region when using the installation program to create the `install-config.yaml` file. The AWS SDK that caused this problem has been updated, and users can now select the `ap-north-east-3` region. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1996544[*BZ#1996544*])

* Previously, installing a private (internal) {product-title} cluster on Azure Stack Hub failed because the installation program did not create the DNS record for the API virtual IP address. This update removes the invalid check that caused this problem. The installation program now correctly creates DNS records for private cluster. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2061549[*BZ#2061549*])

* Previously, uninstalling an IBM Cloud VPC cluster might have caused unexpected results. When a user uninstalled a cluster (cluster 1), the DNS records of another cluster (cluster 2) were removed when either the name of cluster 1 (example) was a subset of the name of cluster 2 (myexample) or both clusters shared a base domain. This update corrects this behavior. Only those resources specific to the cluster that is being uninstalled are now removed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2060617[*BZ#2060617*])

* Previously, Azure Stack Hub did not support any disk types other than Standard_LRS. This update adds the ability to customize disk types, which allows clusters to have a default disk type with no manual customizations. This resulted in making the switch from hard coding the disk type to accepting the inputs from the user and validating it against the Stack Hub APIs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2061544[*BZ#2061544*])

* Previously, when destroying a cluster, the ID of the private route5 hosted zone for the cluster was incorrectly reported when the DNS records were deleted from the hosted zone. This caused incorrect host zone IDs to be reported in the log of the destroyer. This update uses the correct host zone ID in the log. As a result, the log shows the correct hosted zone ID when destroying the DNS record in the base domain's hosted zone. (https://bugzilla.redhat.com/show_bug.cgi?id=1965969[*BZ#1965969*])

* Previously, system proxy settings were not considered when requesting an AWS custom service endpoint. This update configures AWS custom service endpoint validation to consider the system proxy settings along with a `HEAD` request to the AWS custom service endpoint. As a result, the AWS custom service endpoint can be accessed from the user's machine. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2048451[*BZ#2048451*])

* Previously, the installer used any Terraform provider in the `$PATH` on the installer host. Therefore, the installation would fail if there were Terraform providers in the `$PATH` that used an incorrect version or provider instead of the providers embedded in the installation program. With this update, the installation program embeds providers to a known directory and sets Terraform to use the known directory. As a result, installation will be successful because the installation program will always use the providers in the known directory. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1932812[*BZ#1932812*])

* Previously, there was an eventual consistency issue in the AWS Terraform provider when updating new load balancers. Therefore, the installation would fail when trying to access the new load balancers. With this fix, the installer is now updated to an upstream Terraform provider, which ensures eventual consistency. As a result, the installation does not fail. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1898265[*BZ#1898265*])

[discrete]
[id="ocp-4-11-kube-api-server-bug-fixes"]
==== Kubernetes API server

* Previously, long running requests used for streaming were taken into account for the `KubeAPIErrorBudgetBurn` calculation. Consequently, The alert from `KubeAPIErrorBudgetBurn` would be triggered and cause false positives. This update excludes long running requests from the `KubeAPIErrorBudgetBurn` calculation. As a result, false positives are reduced on the `KubeAPIErrorBudgetBurn` metric. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1982704[*BZ#1982704]*)

[discrete]
[id="ocp-4-11-kube-scheduler-bug-fixes"]
==== Kubernetes Scheduler

* With {product-title} 4.11, the hypershift namespace is excluded from eviction when the descheduler is installed on a cluster tha has hypershift enabled. As a result, pods are no longer evicted from the hypershift namespace when the descheduler is installed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2000653[*BZ#2000653]*)

* Previously, resources incorrectly specified the API version in the owner reference of the `kubedescheduler` custom resource (CR). Consequently, the owner reference was invalid, and the affected resources would not be deleted when the `kubedescheduler` CR ran. This update specified the correct API version in all owner references. As a result, all resources with an owner reference to the `kubedescheduler` CR are deleted after the CR is deleted. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1957012[*BZ#1957012]*)

[discrete]
[id="ocp-4-11-machine-config-operator-bug-fixes"]
==== Machine Config Operator

* Because `keyFile` is not configured as the default plug-in for NetworkManager on RHEL nodes, RHEL nodes might not reach the ready state after a reboot.  With this fix, `keyFile` is set as the default NetworkManager plug-in on all cluster nodes. As a result, nodes correctly reach the ready state after a reboot. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2060133[*BZ#2060133*])

* Because vSphere UPI clusters do not set the `PlatformStatus.VSphere` parameter at installation, the parameter was set to `nil`. This caused the MCO logs to be filled with unnecessary and repetitive messages that this parameter cannot have the value `nil`. This fix removes the warning, which was added to resolve a separate issue. As a result, the logs no longer list this message for vSphere UPI installations. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2080267[*BZ#2080267*])

[discrete]
[id="ocp-4-11-management-console-bug-fixes"]
==== Management Console

* Previously, the web console was not properly authenticating permissions when approving `InstallPlans`. This resulted in a possible unhandled error. With this update, permissions were altered for consistency and error messages are now displayed correctly in the web console. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2006067[*BZ#2006067*])

[discrete]
[id="ocp-4-11-monitoring-bug-fixes"]
==== Monitoring

* Before this update, dashboards in the {product-title} web console that contained queries using a container label for `container_fs*` metrics returned no data points because the container labels had been dropped due to high cardinality. This update resolves the issue, and these dashboards now display data as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2037513[*BZ#2037513*])

* Before this update, the `prometheus-operator` component allowed any time value for `ScrapeTimeout` in the config map. If you set `ScrapeTimeout` to a value greater than the `ScrapeInterval` value, Prometheus would stop loading the config map settings and fail to apply all subsequent configuration changes.
With this update, if the `ScrapeTimeout` value specified is greater than the `ScrapeInterval` value, the system logs the settings as invalid, but continues loading the other config map settings.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2037762[*BZ#2037762*])

* Before this update, in the *CPU Utilisation* panel on the *Kubernetes / Compute Resources / Cluster* dashboard in the {product-title} web console, the formula used to calculate the CPU utilization of a node could incorrectly display invalid negative values. With this update, the formula has been updated, and the *CPU Utilisation* panel now shows correct values. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2040635[*BZ#2040635*])

* Before this update, data from the `prometheus-adapter` component could not be accessed during the automatic update that occurs every 15 days because the update process removed old pods before the new pods became available. With this release, the automatic update process now only removes old pods after the new pods are able to serve requests so that data from the old pods continues to be available during the update process.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2048333[*BZ#2048333*])

* Before this update, the following metrics were incorrectly missing from `kube-state-metrics`: `kube_pod_container_status_terminated_reason`, `kube_pod_init_container_status_terminated_reason`, and `kube_pod_status_scheduled_time`. With this release, `kube-state-metrics` correctly exposes these metrics so that they are available. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2050120[*BZ#2050120*])

* Before this update, if invalid write relabel config map settings existed for the `prometheus-operator` component, the configuration would still load all subsequent settings.
With this release, the component checks for valid write relabel settings when loading the configuration. If invalid settings exist, an error is logged, and the configuration loading process stops. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2051470[*BZ#2051470*])

* Before this update, the `init-config-reloader` container for the Prometheus pods requested `100m` of CPU and `50Mi` of memory, even though in practice the container needed fewer resources.
With this update, the container requests `1m` of CPU and `10Mi` of memory. These settings are consistent with the settings of the `config-reloader` container. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2057025[*BZ#2057025*])

* Before this update, when an administrator enabled user workload monitoring, the `user-workload-monitoring-config` config map was not automatically created. Because non-administrator users with the `user-workload-monitoring-config-edit` role did not have permission to create the config map manually, they depended on an administrator to create it. With this update, the `user-workload-monitoring-config` config map is now automatically created when an administrator enables user workload monitoring and is available to edit by users with the appropriate role. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2065577[*BZ#2065577*])

* Before this update, after you deleted a deployment, the Cluster Monitoring Operator (CMO) did not wait for the deletion to be completed, which caused reconciliation errors. With this update, the CMO now waits until deployments are deleted before recreating them, which resolves this issue. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2069068[*BZ#2069068*])

* Before this update, for user workload monitoring, if you configured external labels for metrics in Prometheus, the CMO did not correctly propagate these labels to Thanos Ruler. If you queried external metrics for user-defined projects, not provided by the user workload monitoring instance of Prometheus, you would sometimes not see external labels for these metrics even though you had configured Prometheus to add them. With this update, the CMO now properly propagates the external labels that you configured in Prometheus to Thanos Ruler, and you can see the labels when you query external metrics. Therefore, for user-defined projects, if you queried external metrics not provided by the user workload monitoring instance of Prometheus, you would sometimes not see external labels for these metrics even though you had configured Prometheus to add them. With this update, the CMO now properly propagates the external labels that you configured in Prometheus to Thanos Ruler, and you can see the labels when you query external metrics. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2073112[*BZ#2073112*])

* Before this update, the `tunbr` interface incorrectly triggered the `NodeNetworkInterfaceFlapping` alert. With this update, the `tunbr` interface is now included in the list of interfaces that the alert ignores and no longer causes the alert to trigger incorrectly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2090838[*BZ#2090838*])

* Previously, the Prometheus Operator allowed invalid re-label configurations. With this update, the Prometheus Operator validates re-labeled configurations. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2051407[*BZ#2051407]*)

[discrete]
[id="ocp-4-11-networking-bug-fixes"]
==== Networking

* Previously, when using the bond CNI plug-in for an additional network attachment, it was not compatible with Multus. When the bond CNI plug-in was used in conjunction with the Whereabouts IPAM plug-in for a network attachment definition, assigned IP addresses were incorrectly reconciled. Now a network attachment definition that uses the bond CNI plug-in works correctly with the Whereabouts IPAM plug-in for IP address assignment. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2082360[BZ#2082360])

* Previously, when using the OVN-Kubernetes cluster network provider with multiple default gateways, the wrong gateway was selected, causing the OVN-Kubernetes pods to terminate unexpectedly. Now the correct default gateway is selected such that these pods no longer crashes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2040933[BZ#2040933])

* For clusters using the OVN-Kubernetes cluster network provider, previously if the NetworkManager service restarted on a node, that node lost network connectivity. Now network connectivity survives a restart of the NetworkManager service. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2048352[BZ#2048352])

* Previously a `goroutine` handling cache updates could stall writing to an unbuffered channel while holding a `mutex`. With this update, these race conditions were solved. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2052398[*BZ#2052398*])

* Previously, for ovn-kubernetes, setting up `br-ex` on boot with a bond or team interface caused a mismatch on media access control (MAC) addresses between the `br-ex` and the bond interface. Consequently, on bare metal or some virtual platforms all of the traffic was dropped due to network interface controller (nic) driver dropping traffic due to an unexpected `br-ex` MAC address. With this update, `br-ex` and the bond interface use the same MAC address resulting in no dropped traffic. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2103080[*BZ#2103080*])

* Previously, users with `cluster-reader` role could not read custom resources from kubernetes-nmstate, such as `NodeNetworkConfigurationPolicy`. With this update, users with `cluster-reader` role can read kubernetes-nmstate custom resources. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2022745[*BZ#2022745*])

* Previously, contrack entries for `LoadBalancer` IPs were not removed when the service endpoints were removed causing connections to fail. With this update, contrack entries do not cause connections to fail. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2061002[*BZ#2061002*])

* Previously, a missing `jq package` caused the scale up of a cluster with RHEL nodes to fail on node deployment. With this update,` jq package` is installed on deployment and the scale up of a cluster with RHEL nodes succeeds. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2052393[*BZ#2052393*])

* Previously, OVN-Kubernetes spent excessive time when a service configuration change was made. This caused a noticeable latency in service configuration changes. With this update, OVN-Kubernetes is optimized to reduce the latency of service configuration changes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2070674[*BZ#2070674*])

* Previously, the IP reconciliation CronJob for Whereabouts IPAM CNI would fail due to API connectivity issues, which caused CronJob to intermittently fail. With this update, CronJob launched on Whereabouts IPAM CNI use the api-internal server address and an extended api timeout to prevent these connectivity issues. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2048575[*BZ#2048575*])

* With this update, {product-title} clusters with Kubernetes-NMstate installed now include in the` must-gathers` Kubernetes-NMstate resources. This improves issue handling by including resources from Kubernetes-NMstate in `must-gathers`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2062355[*BZ#2062355*])

* There is currently a known issue with host routes being ignored when load-balancer services are configured with cluster traffic policy. Consequently, egress traffic of load balancer services is steered to the default gateway and not towards the best matching route that is present on the host routing table. As a workaround, set the load balancer type Services to `Local` traffic policy. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2060159[*BZ#2060159*])

* Previously, `PodDisruptionBudget` specifications did not fit for {SNO} clusters, which limited upgrade capabilities due to not all pods being able to be evicted. With this update, `PodDisruptionBudget` specification is adjusted based on cluster topology making Kubernetes-NMState operator capable of upgrading on {SNO} clusters. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2075491[*BZ#2075491*])

* Previously, when setting up `br-ex` bridge on boot, the DHCP client id and IPv6 address generation mode configuration did not work properly causing an unexpected IP address on `br-ex`. With this update, DHCP client id and IPv6 generation mode configuration are now properly set on `br-ex`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2058030[*BZ#2058030*])

[discrete]
[id="ocp-4-11-ne-perf-improvements"]
==== Networking performance improvements
* Previously, a `systemd` service set a default RPS mask according to the reserved CPUs list in the performance profile for all network devices visible from udev excluding virtual devices. A `crio` hook script set the RPS mask of all network devices visible from /sys/devices for guaranteed pods. This resulted in multiple impacts to network performance.
In this update, `systemd` service only sets the default RPS mask for virtual interfaces under /sys/devices/virtual. The `crio` hook script now also excludes physical devices. This configuration mitigates issues such as overloading of processes, lengthy polling intervals, and spikes in latency.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2081852[*BZ#2081852*])

[discrete]
[id="ocp-4-11-node-bug-fixes"]
==== Node

* Previously, the pod manager handled the registration and de-registration of pod secrets and config maps. Because of this, pod secrets would sometimes fail to be mounted within a pod. With this fix, the pod ID is included in the key that the kublet uses to manage registered pods. As a result, secrets are properly mounted as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1999325[*BZ#1999325*])

* Because of a memory leak in the garbage collection process, pods might not be able to start on a node due to lack of memory. With this fix memory is no longer leaking in the garbage collection process and nodes should start as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2065749[*BZ#2065749*])

* Because of a change in the upstream Kubernetes, the kubelet was not running readiness probes on terminated pods. As a result, a load balancer or controller could react more slowly to a terminating pod, which might have resulted in errors. With this fix, readiness probes are again being performed on pod termination. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2089933[*BZ#2089933*])

* Because of a bug, the kubelet could incorrectly reject pods that have `OutOfCpu` errors, if the pods were rapidly scheduled after other pods were reported as complete in the API. With this fix, the kubelet now waits to report the phase of a pod as terminal in the API until all running containers have stopped and no new containers have been started. Short-lived pods may take slightly longer, approximately 1s, to report either success or failure after this change. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2022507[*BZ#2022507*])

* Because recent versions of the `prometheus-adapter` are sending additional pod metrics, the Vertical Pod Autoscaler (VPA) recommender is producing a large number of unnecessary and repetitive messages. With this fix, the VPA recognizes and ignores the extra metrics. As a result, these messages are no longer being produced. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2102947[*BZ#2102947*])

[discrete]
[id="ocp-4-11-openshift-cli-bug-fixes"]
==== OpenShift CLI (oc)

* Previously, `oc` catalog mirroring failed if an older, deprecated image version was used as the source. Now, the image manifest version is detected automatically and mirroring works successfully. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2049133[*BZ#2049133*])

* Previously, it was hard to understand from the logs when fallback inspect occured. The logs are now improved to make this more explicit. As a result, the output of `must-gather run` is much more clear. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2035717[*BZ#2035717*])

* Previously, if you ran `must-gather` with invalid arguments, it did not consistently report the error and instead might attempt to collect data even when that is not possible. Now if `must-gather` is called with invalid options, it provides useful error output. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1999891[BZ#1999891])

* Previously, if the `oc adm catalog mirror` command resulted in errors, it still continued and returned a `0` exit code. A `--continue-on-error` flag is now available that allows users to determine whether the command should continue if there are errors, or exit with a non-zero exit code. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2088483[*BZ#2088483*])

* With this update, a `--subresource` flag was added to the `oc adm policy who-can` command to check who can perform a specified action on a subresource. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1905850[*BZ#1905850*])

* Previously, users were unable to use tab completion for the `oc project` command. Now, hitting tab after `oc project` properly lists projects. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1905850[*BZ#1905850*])

* Previously, startup probes were not removed from debug pods, which could cause issues with the debug pods if the startup probe failed. The `--keep-startup` flag has been added, which is `false` by default, meaning that startup probes are removed by default from debug pods. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2056122[*BZ#2056122*])

* Previously, there was no timeout specified after invoking `oc debug node`, so users were never logged out of the cluster. A `TMOUT` environment variable has been added, so that after the specified time of inactivity, the session is automatically terminated. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2043314[*BZ#2043314*])

* With this update, `oc login` now shows the URL for the web console even when users are logged out. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1957668[*BZ#1957668*])

* Previously, the `oc rsync` command displayed a wrong error output when the container was not found. With this release, the `oc rsync` command displays the correct error message when the specific container is not running. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2057633[*BZ#2057633*])

* Previously, large images could not be pruned if they were new to the cluster. This caused recent images to be omitted when filtering the over sized images. With this release, you can now prune images that exceed the given size. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2083999[*BZ#2083999*])

* Previously, there was a typo in the `gather` script. As a result, insights data was not collected properly. With this release, the typo is corrected and Insights data is now properly collected through the `must-gather`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2106543[*BZ#2106543*])

* Previously, you could not apply the `EgressNetworkPolicy` resource type in your cluster through the `oc` CLI. With this release, you can now create, update, and delete an `EgressNetworkPolicy` resource. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2071614[*BZ#2071614*])

[discrete]
[id="ocp-4-11-container-bug-fixes"]
==== OpenShift containers

[discrete]
[id="ocp-4-11-openshift-controller-manage-bug-fixes"]
==== OpenShift Controller Manager

[discrete]
[id="ocp-4-11-olm-bug-fixes"]
==== Operator Lifecycle Manager (OLM)

* Before this update, invalid subscription labels were created when a resource name exceeded 63 characters. Truncating labels that exceed the 63-character limit resolves the issue, and the subscription resource no longer rejects the Kubernetes API. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2016425[*BZ#2016425*])

* Before this update, catalog source pods for the Marketplace Operator prevented nodes from draining. As a result, the Cluster Autoscaler could not scale down effectively. With this update, adding the `cluster-autoscaler.kubernetes.io/safe-to-evict` annotation to the catalog source pods fixes the issue, and the Cluster Autoscaler can scale down effectively. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2053343[*BZ#2053343*])

* Before this update, the `collect-profiles` job could take a long time to complete in certain circumstances, such as when a pod could not be scheduled. As a result, if enough jobs were scheduled but unable to run, the number of scheduled jobs exceeded pod quota limits. With this update, only one `collect-profiles` pod exists at a time, and the `collect-profiles` job does not exceed pod quota limits. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2055861[*BZ#2055861*])

* Before this update, the package server was not aware of pod topology when defining its leader election duration, renewal deadline, and retry periods. As a result, the package server strained topologies with limited resources, such as single-node environments. This update introduces a `leaderElection` package that sets reasonable lease duration, renewal deadlines, and retry periods. This fix reduces strain on clusters with limited resources. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2048563[*BZ#2048563*])

* Previously, there was a bad catalog source in the `openshift-marketplace` namespace. Because of this, all subscriptions were blocked. With this update, if there is a bad catalog source in the `openshift-marketplace` namespace, users can subscribe to an operator from a quality catalog source of their own namespace with the original annotation. As a result, if there is a bad catalog source in the local namespace, the user cannot subscribe to any operator in the namespace. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2076323[*BZ2076323])

* Previously, info-level logs were generated during `operator-marketplace` project polling, which caused log spam. This update uses the command line flag to reduce the log line to the debug level, and adds more control of the log levels for the user. As a result, this reduces log spam. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2057558[*BZ#2057558*])

* Previously, each component managed by the Cluster Version Operator (CVO) consisted of YAML files defined in the `/manifest` directory in the root of a project's repo. When removing a YYAML file from the `/manifest` directory, you needed to add the `release.openshift.io/delete: “true”` annotation, otherwise the CVO would not delete the resources from the cluster. This update reintroduces any resources that were removed from the `/manifest` directory and adds the `release.openshift.io/delete: “true”` annotation so that the CVO cleans up the resources. As a result, resources that are no longer required for the OLM component are removed from the cluster. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1975543[*BZ#1975543*])

* Previously, the `CheckRegistryServer` function used by gRPC catalog sources did not confirm the existence of the service account associated with the catalog source. This caused the existence of an unhealthy catalog source with no service account. With this update, the gRPC `CheckRegistryServer` function checks if the service account exists and recreates the service if it is not found. As a result, the OLM recreates service accounts owned by gRPC catalog sources if they do not exist. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2074612[*BZ#2074612*])

* Previously, in an error message that occurred when users ran `opm index prune` against a file-based catalog image, imprecise language made it unclear that this command does not support that catalog format. This update clarifies the error message so users understand that the command `opm index prune` only supports SQLite-based images. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2039135[*BZ#2039135*])

* Previously, there was a broken thread safety around the Operator API. Consequently, Operator resources were not properly deleted. With this update, Operator resources are correctly deleted. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2015023[*BZ#2015023*])

* Previously, pod failures were artificially extending the validity period of certificates causing them to incorrectly rotate. With this update, the certificate validity period is correctly determined and the certificates are correctly rotated. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2020484[*BZ#2020484*])

* In {product-title} {product-version} the default cluster-wide pod security admission policy is set to `baseline` for all namespaces and the default warning level is set to `restricted`. Before this update, Operator Lifecycle Manager displayed pod security admission warnings in the `operator-marketplace` namespace. With this fix, reducing the warning level to `baseline` resolves the issue. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2088541[*BZ#2088541*])

[discrete]
[id="ocp-4-11-openshift-operator-sdk-bug-fixes"]
==== Operator SDK

* Before this update, the Operator SDK used upstream images rather than downstream supported images to scaffold Hybrid Helm-based Operators. With this update, the Operator SDK uses supported downstream images to scaffold Hybrid Helm-based Operators. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2039135[*BZ#2039135*])

* With {product-title} 4.11, Operator SDK allows for `arm64` Operator images to be built. As a result, Operator SDK now supports building Operator images that target `arm64`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2035899#[*BZ#2035899*])

[discrete]
[id="ocp-4-11-openshift-api-server-bug-fixes"]
==== OpenShift API server

* Because multiple Authentication Operator controllers were synchronizing at the same time, the Authentication Operator was taking too long to react to changes to its configuration. This feature adds jitter to the regular synchronization periods so that the Authentication Operator controllers do not race for resources. As a result, it now takes less time for the Authentication Operator to react to configuration changes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1958198[*BZ#1958198*])

* With {product-title} 4.11, authentication attempts from external identity providers are now logged to the audit logs. As a result, you can view successful, failed, and errored login attempts from external identity providers in the audit logs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2086465[*BZ#2086465*])

[discrete]
[id="ocp-4-11-openshift-update-service-bug-fixes"]
==== OpenShift Update Service

[discrete]
[id="ocp-4-11-rhcos-bug-fixes"]
==== {op-system-first}

* Before this update, if a machine was booted through PXE and the `BOOTIF` argument was on the kernel command line, the machine would boot with DHCP enabled on only a single interface. With this update, the machine boots with DHCP enabled on all interfaces even if the `BOOTIF` argument is provided. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2032717[BZ#2032717])

* Previously, nodes that were provisioned from VMware OVA images did not delete the Ignition config after initial provisioning. Consequently, this created security issues when secrets are stored within the Ignition config. With this update, the Ignition config is now deleted from the VMware hypervisor after initial provisioning on new nodes and when upgrading from a previous {product-title} release on existing nodes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2082274[BZ#2082274])

* Previously, any arguments provided to the `toolbox` command were ignored when the command was first invoked. This fix updates the toolbox script to initiate the `podman container create` command followed by the `podman start` and `podman exec` commands. It also modifies the script to handle multiple arguments and whitespaces as an array. As a result, the arguments passed to the `toolbox` command are executed every time as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2039589[*BZ#2039589*])

[discrete]
[id="ocp-4-11-performance-bug-fixes"]
==== Performance

[discrete]
[id="ocp-4-11-performance-addon-operator-bug-fixes"]
==== Performance Addon Operator

[discrete]
[id="ocp-4-11-routing-bug-fixes"]
==== Routing

* Previously, the Ingress Operator did not validate whether a Kubernetes service object in the OpenShift Ingress namespace was created or owned by the Ingress Controller it was trying to reconcile with. Therefore, the Ingress Operator would modify or remove Kubernetes services that had the same name and namespace, regardless of ownership, causing unexpected behavior. With this update, the Ingress Operator can now check the ownership of existing Kubernetes services before attempting to modify or remove services. If ownership does not match, the Ingress Operator shows an error and does not take any action. As a result, the Ingress Operator cannot cannot modify or delete a custom Kubernetes service with the same name as the OpenShift Ingress namespace that it wants to modify or remove. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2054200[*BZ#2054200*])

* Previously, {product-title} 4.8 added an API for customizing platform routes. This API includes status and spec fields in the cluster ingress configuration for reporting the current host names of customizable routes and the user's desired host names for these routes, respectively. The API also defined constraints for these values. These constraints were restrictive and excluded some valid potential host names. Consequently, the restrictive validation for the API prevented users from specifying custom host names that should have been permitted and prevented users from being able to install clusters with domains that should have been permitted. With this update, the constraints on host names were relaxed to allow all host names that are valid for routes and {product-title} allows users to use cluster domains with TLDs that contain decimal digits. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2039256[*BZ#2039256*])

* Previously, the Ingress Operator did not check whether Ingress Controllers configured with cluster `spec.domain` parameter matched the `spec.baseDomain` parameter. This caused the Operator to create DNS records and set `DNSManaged` conditions to `false`. With this fix, the Ingress Operator now checks whether the `spec.domain` parameter matches with the cluster `spec.baseDomain`. As a result, for custom Ingress Controllers, the Ingress Operator does not create DNS records and sets `DNSManaged` conditions to false. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2041616#[*BZ#2041616*])

* Previously, in OpenShift Container Platform 4.10, the HAProxy must-gather function could take up to an hour to run. This can happen when routers in the terminating state delay the `oc cp` command. The delay lasts until the pod is terminated. With the new release, a 10 minute limit on the `oc op` command prevents longer delays. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2104701[*BZ#2104701*])

* Previously, the Ingress Operator did not clear the route status when Ingress Controllers were deleted, showing that the route was still in the operator after its deletion. This fix clears the route status when an Ingress Controller is deleted, resulting in the route being cleared in the operator after its deletion. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1944851#[*BZ#1944851*])

* Previously, the output for the `oc explain router.status.ingress.conditions` command explain route status showed `Currently only Ready` rather than `Admitted` due to incorrect wording in the Application Programming Interface (API). This fix corrects the wording in the API. As a result, the command output is correct. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2041133#[*BZ#2041133*])

[discrete]
[id="ocp-4-11-samples-bug-fixes"]
==== Samples

[discrete]
[id="ocp-4-11-scalability-and-performance-bug-fixes"]
==== Scalability and performance

* Before this update, SRO installed Node Feature Discovery (NFD) by default, regardless of whether NFD had already been installed. If NFD had been installed, this would cause the SRO deployment to fail. SRO no longer deploys NFD by default.

[discrete]
[id="ocp-4-11-storage-bug-fixes"]
==== Storage

[discrete]
[ids="ocp-4-11-telco-edge-bug-fixes"]
==== Telco Edge

[discrete]
[id="ocp-4-11-web-console-admin-perspective-bug-fixes"]
==== Web console (Administrator perspective)

[discrete]
[id="ocp-4-11-web-console-developer-perspective-bug-fixes"]
==== Web console (Developer perspective)

* Before this update, the *Git Import* form displayed an error message when an invalid devfile repository (older than devfile v2.2) is entered. With this update, the error message states that devfiles older than v2.2 are not supported. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2046435[*BZ#2046435*])

* Before this update, if the *ConsoleLink CR* (openshift-blog) was not available in the cluster, the blog link was undefined. Clicking on the blog link did not redirect to the OpenShift blog. With this update, a fall back link to https://developers.redhat.com/products/openshift/whats-new is added even if the *ConsoleLink CR* (openshift-blog) is not present in the cluster. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2050637[*BZ#2050637*])

* Before this update, the API version for the kafka CR was updated. This version did not support the old version, so an empty *Bootstrap server* was displayed on *Create Event Source - KafkaSource* even if it was created. With this update, the updated APIs for the Kafka CR support the old versions and renders the *Bootstrap server* list in *Create Event Source - KafkaSource* form. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2058623[*BZ#2058623*])

* Before this update, when you used the *Import from Git* form to import a private Git repository, the correct import type and a builder image were not identified because the secret to fetch the private repository details was not decoded. With this update, the *Import from Git* form decodes the secret to fetch the private repository details. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2053501[*BZ#2053501*])

* Before this update, from the developer perspective, the *Observe* dashboard opened for the most recently viewed workload rather than the one you selected in the *Topology* view. This issue happens because the session prefers the redux store instead of the query parameters in the URL. With this update, the *Observe* dashboard renders components based on the query parameters in the URL. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2052953[*BZ#2052953*])

* Before this update, the *Pipeline* used to start with the hardcoded value `gp2` as the default storage class even if it did not exist on the cluster. With this update, you can use the default specified storage class name instead of a hardcoded value. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2084635[*BZ#2084635*])

* Before this update, while running high-volume pipeline logs, the auto-scroll functionality does not work and logs display older messages. Running high-volume pipeline logs generates a large number of calls to the `scrollIntoView` method. With this update, high-volume pipeline logs do not generate any calls to the `scrollIntoView` method and gives a smooth auto-scroll functionality. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2014161[*BZ#2014161*])

* Before this update, when creating a *RoleBinding* using the *Create RoleBinding* form, the subject name was mandatory. A missing subject name fails to load the *Project Access* tab. With this update, the *RoleBinding* without the *Subject Name* property is not listed in the *Project Access* tab. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2051558[*BZ#2051558*])

* Before this update, the sink and trigger for event sources showed all the resources, even though those are standalone or part of backing the `k-native service`, `Broker`, or `KameletBinding`. The addressed resources used to show in the sink dropdown list. With this update, a filter has been added to show only standalone resources as sink. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2054285[*BZ#2054285*])

* Before this update, the empty tabs in the sidebar of the topology view were not filtered out before rendering. This displayed invalid tabs for *Workloads* in the topology view. With this update, the empty tabs are filtered properly before rendering. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2049483[*BZ#2049483*])

* Before this update, when a pipeline is started using the *Start Last Run* button, the `started-by` annotation of the created `PipelineRun` was not updated to the correct username so the triggered by section did not show the correct username. With this update, the `started-by` annotation value is updated to the correct username and the triggered by section shows the username of the correct user that started the pipeline. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2046618[*BZ#2046618*])

* Before this update, the `ProjectHelmChartRepository` CR does not show up in the cluster. Consequently, the API schema for this CR has not been initialized in the cluster yet. With this update the `ProjectHelmChartRepository` shows up in the cluster. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2054197[*BZ#2054197*])

* Before this update, when you navigate using a keyboard in the topology, the selected items were not highlighted. With this update, navigation using a keyboard highlights and updates styles to the selected items. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2039277[*BZ#2039277*])

* Before this update, the layout of the web terminal opened outside of the default view and could not be resized. With this update, the web terminal opens inside the default view and resizes properly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2022253[*BZ#2022253*])

* Before this update, some of the sidebar items did not contain namespace context. Consequently, when links were opened from another browser, or opening links from a different active namespace, the web console does not switch to the correct namespace. With this update, the correct namespace is selected when opening the URL. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2039647[*BZ#2039647*])

[id="ocp-4-11-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *-*: _Not Available_
* *DEP*: _Deprecated_

//TODO: remove anything that has been GA since 4.7?

.Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |OCP 4.9 |OCP 4.10 | OCP 4.11

|Precision Time Protocol (PTP) hardware configured as ordinary clock
|GA
|GA
|GA

|PTP single NIC hardware configured as boundary clock
|-
|TP
|GA

|PTP dual NIC hardware configured as boundary clock
|-
|-
|TP

|PTP events with ordinary clock
|TP
|GA
|GA

|PTP events with boundary clock
|-
|TP
|GA

|`oc` CLI plug-ins
|GA
|GA
|GA

|Shared Resources CSI Driver and Build CSI Volumes in OpenShift Builds
|-
|TP
|TP

|Service Binding
|TP
|GA
|GA

|Raw Block with Cinder
|GA
|GA
|GA

|CSI volume expansion
|TP
|TP
|

|CSI AliCloud Disk Driver Operator
|-
|GA
|GA

|CSI Azure Disk Driver Operator
|TP
|GA
|GA

|CSI Azure File Driver Operator
|-
|TP
|

|CSI Azure Stack Hub Driver Operator
|GA
|GA
|GA

|CSI GCP PD Driver Operator
|GA
|GA
|GA

|CSI IBM VPC Block Driver Operator
|-
|GA
|GA

|CSI OpenStack Cinder Driver Operator
|GA
|GA
|GA

|CSI AWS EBS Driver Operator
|GA
|GA
|GA

|CSI AWS EFS Driver Operator
|TP
|GA
|GA

|CSI automatic migration
|TP
|TP
|

|CSI inline ephemeral volumes
|TP
|TP
|

|CSI vSphere Driver Operator
|TP
|GA
|GA

|Shared Resource CSI Driver
|-
|TP
|

|Automatic device discovery and provisioning with Local Storage Operator
|TP
|TP
|

|OpenShift Pipelines
|GA
|GA
|GA

|OpenShift GitOps
|GA
|GA
|GA

|OpenShift sandboxed containers
|TP
|GA
|GA

|Vertical Pod Autoscaler
|GA
|GA
|GA

|Cron jobs
|GA
|GA
|GA

|PodDisruptionBudget
|GA
|GA
|GA

|Adding kernel modules to nodes with kvc
|TP
|TP
|TP

|Egress router CNI plug-in
|GA
|GA
|GA

|Scheduler profiles
|GA
|GA
|GA

|Non-preempting priority classes
|TP
|TP
|TP

|Kubernetes NMState Operator
|TP
|GA
|GA

|Assisted Installer
|TP
|TP
|TP

|AWS Security Token Service (STS)
|GA
|GA
|GA

|`kdump` on `x86_64` architecture
|TP
|TP
|GA

|`kdump` on `arm64` architecture
|-
|-
|TP

|`kdump` on `s390x` architecture
|TP
|TP
|TP

|`kdump` on `ppc64le` architecture
|TP
|TP
|TP

|OpenShift Serverless
|GA
|GA
|GA

|OpenShift on ARM platforms
|-
|GA
|GA

|Serverless functions
|TP
|TP
|TP

|Data Plane Development Kit (DPDK) support
|GA
|GA
|GA

|Memory Manager feature
|TP
|TP
|

|CNI VRF plug-in
|GA
|GA
|GA

|Cluster Cloud Controller Manager Operator
|GA
|GA
|GA

|Cloud controller manager for Alibaba Cloud
|-
|TP
|

|Cloud controller manager for Amazon Web Services
|TP
|TP
|

|Cloud controller manager for Google Cloud Platform
|-
|TP
|

|Cloud controller manager for IBM Cloud
|-
|GA
|GA

|Cloud controller manager for Microsoft Azure
|TP
|TP
|

|Cloud controller manager for Microsoft Azure Stack Hub
|GA
|GA
|GA

|Cloud controller manager for {rh-openstack-first}
|TP
|TP
|

|Cloud controller manager for VMware vSphere
|-
|TP
|

|Driver Toolkit
|TP
|TP
|TP

|Special Resource Operator (SRO)
|TP
|TP
|TP

|Simple Content Access
|TP
|GA
|GA

|Node Health Check Operator
|TP
|TP
|GA

|Network bound disk encryption (Requires Clevis, Tang)
|GA
|GA
|GA

|MetalLB Operator
|GA
|GA
|GA

|CPU manager
|GA
|GA
|GA

|Pod-level bonding for secondary networks
|-
|TP
|

|IPv6 dual stack
|GA
|GA
|GA

|Multicluster console
|-
|TP
|

|Selectable Cluster Inventory
|-
|TP
|

|Hyperthreading-aware CPU manager policy
|-
|TP
|

|Dynamic Plug-ins
|-
|TP
|

|Hybrid Helm Operator
|-
|TP
|

|Alert routing for user-defined projects monitoring
|-
|TP
|GA

|Disconnected mirroring with the oc-mirror CLI plug-in
|-
|TP
|GA

|Mount shared entitlements in BuildConfigs in RHEL
|-
|TP
|

|Mount shared secrets in RHEL
|-
|GA
|GA

|Support for {rh-openstack} DCN
|-
|TP
|

|Support for external cloud providers for clusters on {rh-openstack}
|-
|TP
|TP

|OVS hardware offloading for clusters on {rh-openstack}
|-
|TP
|GA

|External DNS Operator
|-
|TP
|TP

|Web Terminal Operator
|TP
|GA
|GA

|Alerting rules based on platform monitoring metrics
|-
|-
|TP

|AWS Load Balancer Operator
|-
|-
|TP

|Node Observability Operator
|-
|-
|TP

|Java-based Operator
|-
|-
|TP

|Hosted control planes for {product-title}
|-
|-
|TP

|Managing machines with the Cluster API
|-
|-
|TP
|====

[id="ocp-4-11-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.8 and beyond! This needs some SME/QE review before being updated for 4.11. Need to check if KI should be removed or should stay.
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to 4.8, you can either revoke or continue to allow unauthenticated access. Unless there is a specific need for unauthenticated access, you should revoke it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP `403` errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

* In the monitoring stack, if you have enabled and deployed a dedicated Alertmanager instance for user-defined alerts, you cannot silence alerts in the *Developer* perspective in the {product-title} web console.
No workaround exists for this issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2100860[*BZ#2100860*])

* On a newly installed {product-title} {product-version} cluster, platform monitoring alerts do not have the `openshift_io_alert_source="platform"` label.
This issue does not affect clusters upgraded from a previous minor version.
No workaround exists for this issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2103127[*BZ#2103127*])

* In the {rh-openstack-first}, a potential issue can affect Kuryr when the ports pools are populated with a variety of bulk port creation requests made at the same time. During these bulk requests, if the IP allocation for one of the IP addresses fails, the Neutron retries the operation for all ports. This issue was resolved in a previous errata; however, Red Hat suggests setting a small value for the ports pools batch to avoid large bulk port creation requests. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2024690[*BZ2024690*])

* The oc-mirror CLI plug-in cannot mirror {product-title} catalogs earlier than version 4.9. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2097210[*BZ#2097210*])

* The oc-mirror CLI plug-in can fail to upload an image set to the target mirror registry if the `archiveSize` value in the image set configuration is smaller than the container image size, causing the catalog directory to span multiple archives. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2106461[*BZ#2106461*])

* In {product-title} 4.11, the MetalLB Operator scope has changed from namespace to cluster and this results in upgrading from a prior version failing.
+
As a workaround, remove the previous version of the MetalLB Operator. Do not delete the namespace or the instance of the MetalLB custom resource, and deploy the new Operator version. This keeps MetalLB running and configured.
+
For more information, see xref:../networking/metallb/metallb-upgrading-operator.adoc#metallb-upgrading-operator[Upgrading the MetalLB Operator]. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2100180[*BZ#2100180*])

* The OpenShift CLI (`oc`) for {product-title} 4.11 does not work properly on macOS due to a change in error handling of untrusted certificates in Go 1.18 libraries. Due to this change, `oc login` and other `oc` commands can fail with a `certificate is not trusted` error without proceeding further when running on macOS. Until the error handling is properly fixed in Go 1.18 (tracked by link:https://github.com/golang/go/issues/52010[Go issue #52010]), the workaround is to use the {product-title} 4.10 `oc` CLI instead. Note that when using the {product-title} 4.10 `oc` CLI with an {product-title} 4.11 cluster, it is no longer possible to get a token by using the `oc serviceaccounts get-token <service_account>` command. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2097830[*BZ#2097830*]) (link:https://bugzilla.redhat.com/show_bug.cgi?id=2109799[*BZ#2109799*])

* There is currently a known issue in the *Add Helm Chart Repositories* form to extend the Developer Catalog of a project. The *Quick Start* guides shows that you can add the `ProjectHelmChartRepository` CR in the desired namespace whereas it does not mention that to perform this you need permission from the kubeadmin. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2054197[BZ#2054197])

* There is currently a known issue. If you create an instance of `ProjectHelmChartRepository` custom resource (CR) that uses TLS verification, you cannot list the repositories and perform Helm-related operations. There is currently no workaround for this issue. (link:https://issues.redhat.com/browse/HELM-343[*HELM-343*])

* If the pruner fails, the Image Registry Operator is degraded until the pruner successfully runs. By default, the pruner runs once a day. As a workaround, you can give it more opportunities to run successfully by configuring it to run more often. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1990125[*BZ#1990125*])

* When running {product-title} on bare-metal IBM Power, there is a known issue that the Petitboot bootloader is unable to populate boot configurations for some {op-system} live images. In such cases, after booting nodes with PXE to install {op-system}, the expected live image disk configuration might not be visible.
+
As a workaround, you can boot manually with `kexec` from the Petitboot shell.
+
Identify the disk that holds the live image, in this example `nvme0n1p3`, and run the following commands:
+
[source,bash]
----
# cd /var/petitboot/mnt/dev/nvme0n1p3/ostree/rhcos-*/
# kexec -l vmlinuz-*.ppc64le -i initramfs-*.img -c "ignition.firstboot rd.neednet=1 ip=dhcp $(grep options /var/petitboot/mnt/dev/nvme0n1p3/loader/entries/ostree-1-rhcos.conf | sed 's,^options ,,')" && kexec -e
----
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2107674[BZ#2107674])

* In a disconnected environment, SRO does not try to fetch DTK from the main registry. It fetches from the mirror registry instead. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2102724[*BZ#2102724*])

* The process counter displays incorrect information for the `phc2sys` process on the interface where `phc2sys` is not running. There is currently no workaround for this issue. (link:https://issues.redhat.com/browse/OCPBUGSM-46005[*OCPBUGSM-46005*])

* When a network interface controller (NIC) in a node with a dual NIC PTP configuration is shut down, a faulty event is generated for both PTP interfaces. There is curently no workaround for this issue. (link:https://issues.redhat.com/browse/OCPBUGSM-46004[*OCPBUGSM-46004*])

* In low-band systems, the system clock does not synchronize with the PTP ordinary clock after the grandmaster clock is disconnected for a few hours and recovered. There is currently no workaround for this issue. (link:https://issues.redhat.com/browse/OCPBUGSM-45173[*OCPBUGSM-45173*])

[id="ocp-4-11-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating-cluster-within-minor.adoc#updating-cluster-within-minor[updating your cluster] properly.
====

//Update with relevant advisory information
[id="ocp-4-11-0-ga"]
=== RHSA-2022:0056 - {product-title} 4.11.0 image release, bug fix, and security update advisory

Issued: TBD

{product-title} release 4.11.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2022:0056[RHSA-2022:0056] advisory. A secondary set of bug fixes can be found in the link:https://access.redhat.com/errata/RHEA-2022:0748[RHEA-2022:0748] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2022:0055[RHSA-2022:0055] advisory.

Space precluded documenting all of the container images for this release in the advisory. See the following article for notes on the container images in this release:

link:https://access.redhat.com/solutions/[{product-title} 4.11.0 container image list]
